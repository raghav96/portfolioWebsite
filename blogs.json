{"status":"ok","feed":{"url":"https://medium.com/feed/@raghavism","title":"Stories by Raghav Ravisankar on Medium","link":"https://medium.com/@raghavism?source=rss-226ffefb5260------2","author":"","description":"Stories by Raghav Ravisankar on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*rKKWB7693PplgCq-Ijd7Hg.jpeg"},"items":[{"title":"The Football Derby \u2014 A Data Science Story","pubDate":"2018-08-21 21:55:35","link":"https://towardsdatascience.com/the-football-derby-a-data-science-story-f903d2c9e2ff?source=rss-226ffefb5260------2","guid":"https://medium.com/p/f903d2c9e2ff","author":"Raghav Ravisankar","thumbnail":"","description":"\n<p>So this is my next article in a series of Medium posts this summer where I have worked on building my skills in data science to work towards a career as a data scientist.</p>\n<p>Working with football data is something that I wanted to do for a long time and this time, I have taken the opportunity to build an analysis of different sequences of play and the game at large using a dataset with positions of soccer players to come up with some interesting insights about the sequence of play or the game\u00a0itself.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*uEYgYwJmH9-05O71\"><figcaption>Photo by <a href=\"https://unsplash.com/@willianjusten?utm_source=medium&amp;utm_medium=referral\">Willian Justen de Vasconcellos</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>I acquired this dataset from STATS, from their website <a href=\"https://www.stats.com/data-science/\">https://www.stats.com/data-science/</a>. You can fill out the request form for the soccer data from their website in order to send a request for the data, after which they would send you an agreement to sign before providing the dataset to\u00a0you.</p>\n<p>The Jupyter notebook for this analysis is\u00a0<a href=\"https://www.github.com/raghav96/datascience\">here</a>.</p>\n<h3>Starting off</h3>\n<p>So to sum the things that I intend to do in this\u00a0analysis</p>\n<ul>\n<li>visualize sequences of play in football,</li>\n<li>display statistics about sequence with respect to a single sequence of\u00a0play,</li>\n<li>visualize positions of players on the pitch throughout the\u00a0game,</li>\n<li>Apply KMeans clustering on the dataset in order to come up with interesting insights about the\u00a0data</li>\n</ul>\n<h3>Dataset Structure</h3>\n<p>The dataset contains multiple columns of sequences (\u2018sequence_1\u2019 to \u2018sequence_18\u2019) of play from actual football games in which each players and the balls positions are\u00a0recorded</p>\n<p>Let us define the columns in the data based on available information above: first 22 columns are X,Y positions of defensive players, second 22 columns are X,Y columns of offensive players, last 2 columns are X,Y position of the ball in\u00a0play</p>\n<p>TLDR; This information about the dataset came with the dataset, read this if you would like detailed information about the\u00a0dataset</p>\n<blockquote>Included in the data set are a training set with 7500 sequences and two separate set of sequences for testing. Python users can load the training data (in pickle format) into a dictionary, the key for each sequence would be \u201c\u200b<strong><em>sequence_n</em></strong>\u200b\u201d,where n is anywhere from 1 to\u00a07500.</blockquote>\n<blockquote>Each sequence contains a segment of tracking data corresponding to actual game play from a recent professional soccer league. The format of each sequence is as\u00a0follows:</blockquote>\n<blockquote>- Each sequence is a matrix (numpy 2D array) with 46 columns. Each row contains 23 pairs of (x,y) coordinates of 22 players from both teams and the ball at frequency of\u00a010Hz.</blockquote>\n<blockquote>- In the first 22 columns are 11 (x,y) pairs of defense team. The following 22 columns are coordinates of attacking team (defined as the team with consecutive possession of the ball). The last 2 columns are coordinates of the\u00a0ball.</blockquote>\n<blockquote>- Each set of 22 columns for both attacking and defending team consist of (x,y) pair for the goalkeeper, followed by 10 consecutive (x,y) pairs for the other 10 teammates. The identities and teams vary from sequence to sequence. However, within each sequence, the identity is consistent. Thus concretely, out of the 46 columns from each sequence, we know that the first 2 columns represent the coordinate of defense team\u2019s keeper. Columns 2 to 22 contain 10 consecutive (x,y) pairs of other defensive players. Columns 23 and 24 carry x and y coordinates of the attacking team\u2019s keeper. Columns 25 to 44 contain 10 consecutive (x,y) pairs of other attacking players. Columns 45 and 46 carry x and y coordinates of the\u00a0ball.</blockquote>\n<blockquote>- The coordinates generally belong to the [-52.5 meter, +52.5 meter] range along the x-axis, and [-34 meter, +34 meter] range along the y-axis, with the very center of the pitch being [0,0]. So for example, to normalize the data to the range [-1,+1], one can simply divide the x-columns by 52.5 and y-columns by 34 (this effectively will re-scale the pitch, which roughly corresponds to soccer field of size 105mx70m, from a rectangular box to a square\u00a0box)</blockquote>\n<blockquote>- The coordinates were also adjusted so that the attacking team will moves from left to right, meaning the defending team defends the goal on the right hand\u00a0side.</blockquote>\n<blockquote>- In aggregate, the data set amounts to equivalently and approximately 45 games worth of playing time, with redundant and \u201cdead\u201d situations removed.</blockquote>\n<h3>Acquiring and Wranging\u00a0Data</h3>\n<p>To start off this analysis I imported the dataset into the Jupyter notebook. The data was in the \u2018.pkl\u2019 format, which was new to me, however a quick Google search revealed the <em>pd.read_pickle</em> function in\u00a0pandas.</p>\n<pre>import pandas as pd<br>import seaborn as sns<br>import matplotlib.pyplot as plt<br>from matplotlib import patches<br>import numpy as np<br>%matplotlib inline</pre>\n<pre>data = pd.read_pickle('football/test_data_1.pkl')</pre>\n<p>Let us define the columns in the data based on available information above: first 22 columns are X,Y positions of defensive players, second 22 columns are X,Y columns of offensive players, last 2 columns are X,Y position of the ball in\u00a0play</p>\n<pre>columns = ['GKDef_X', 'GKDef_Y', 'Def1_X', 'Def1_Y','Def2_X', 'Def2_Y','Def3_X', 'Def3_Y','Def4_X', 'Def4_Y','Def5_X', 'Def5_Y', 'Def6_X', 'Def6_Y', 'Def7_X', 'Def7_Y','Def8_X', 'Def8_Y','Def9_X', 'Def9_Y','Def10_X', 'Def10_Y', 'GKAtt_X','GKAtt_Y','Att1_X','Att1_Y','Att2_X','Att2_Y','Att3_X','Att3_Y','Att4_X','Att4_Y','Att5_X','Att5_Y','Att6_X','Att6_Y','Att7_X','Att7_Y','Att8_X','Att8_Y','Att9_X','Att9_Y','Att10_X','Att10_Y','Ball_X','Ball_Y']</pre>\n<p>From here, its now time to make a pandas DataFrame to hold the\u00a0values</p>\n<pre>test_sequence = 'sequence_1'<br>df = pd.DataFrame(data[test_sequence], columns = columns)<br>df.head(5)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0Zxuxm78gJtbtJZQ37e4XQ.png\"><figcaption>Result from the code above, we can see now the data split into the respective columns in a DataFrame</figcaption></figure><h4>Visualize football pitch and movement of the\u00a0ball</h4>\n<p>In this part of the analysis, we are going to convert what we have above into a visualization of the position of the ball. Let us the use the columns we designed above to convert the data into a DataFrame and display the trajectory of the ball for a test sequence. Here is code for ball position for a single point of time in a sequence.</p>\n<pre># Obtaining the positions occupied by the ball<br>ball_pos = df[['Ball_X','Ball_Y']].values.tolist()<br>ball_pos[0]</pre>\n<pre># Output = [-19.961977005004883, 3.843478202819824]</pre>\n<p>Here is the code for building the pitch, here I used matplotlib to build it out, and used code from this\u00a0<a href=\"https://fcpython.com/visualisation/drawing-pitchmap-adding-lines-circles-matplotlib\">article</a>.</p>\n<pre># Plotting the ball along with the position of the ball<br># Run with createPitch([]) to just display the pitch<br>def createPitch(ball_pos, defense_pos, attack_pos, size=0.2):<br>    #Create figure<br>    fig=plt.figure()<br>    ax=fig.add_subplot(1,1,1)</pre>\n<pre>#Pitch Outline &amp; Centre Line<br>    plt.plot([-52.5,-52.5],[-35,35], color=\"black\")<br>    plt.plot([-52.5,52.5],[35,35], color=\"black\")<br>    plt.plot([52.5,52.5],[35,-35], color=\"black\")<br>    plt.plot([52.5,-52.5],[-35,-35], color=\"black\")<br>    plt.plot([0,0],[-35,35], color=\"black\")<br><br>    #Left Penalty Area<br>    plt.plot([-39.5,-39.5],[16,-16],color=\"black\")<br>    plt.plot([-52.5,-39.5],[16,16],color=\"black\")<br>    plt.plot([-39.5,-52.5],[-16,-16],color=\"black\")<br><br>    #Right Penalty Area<br>    plt.plot([39.5,39.5],[16,-16],color=\"black\")<br>    plt.plot([52.5,39.5],[16,16],color=\"black\")<br>    plt.plot([39.5,52.5],[-16,-16],color=\"black\")<br><br>    #Left 6-yard Box<br>    plt.plot([-52.5,-47.5],[-10,-10],color=\"black\")<br>    plt.plot([-47.5,-47.5],[-10,10],color=\"black\")<br>    plt.plot([-52.5,-47.5],[10,10],color=\"black\")<br><br>    #Right 6-yard Box<br>    plt.plot([52.5,47.5],[-10,-10],color=\"black\")<br>    plt.plot([47.5,47.5],[-10,10],color=\"black\")<br>    plt.plot([52.5,47.5],[10,10],color=\"black\")<br><br>    #Create Arc and add it to our plot<br>    leftArc = patches.Arc((-45.5,0),height=18.3,width=18.3,angle=0,theta1=310,theta2=50,color=\"black\")<br>    rightArc = patches.Arc((45.5,0),height=18.3,width=18.3,angle=0,theta1=130,theta2=230,color=\"black\")</pre>\n<pre>#Assign circles to variables - do not fill the centre circle!<br>    centreCircle = plt.Circle((0,0),10,color=\"black\",fill=False)<br>    centreSpot = plt.Circle((0,0),0.8,color=\"black\")<br>    leftPenSpot = plt.Circle((-43.5,0),0.4, color=\"black\")<br>    rightPenSpot = plt.Circle((43.5,0),0.4, color=\"black\")<br><br>    #Draw Arcs<br>    ax.add_patch(leftArc)<br>    ax.add_patch(rightArc)</pre>\n<pre>#Draw the circles to our plot<br>    ax.add_patch(centreCircle)<br>    ax.add_patch(centreSpot)<br>    ax.add_patch(leftPenSpot)<br>    ax.add_patch(rightPenSpot)<br><br>    # Plotting the ball<br>    for pos in attack_pos:<br>        locBall = plt.Circle((pos[0],pos[1]),size,color=\"red\")<br>        ax.add_patch(locBall)<br>    for pos in defense_pos:<br>        locBall = plt.Circle((pos[0],pos[1]),size,color=\"blue\")<br>        ax.add_patch(locBall)<br>    for pos in ball_pos:<br>        locBall = plt.Circle((pos[0],pos[1]),size,color=\"green\")<br>        ax.add_patch(locBall)<br><br><br>createPitch(ball_pos, [], [])</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/752/1*AAZhd8FFVRVWyMPiSUnapA.png\"><figcaption>Result from the code above, position of the\u00a0ball</figcaption></figure><h4>Visualizing an entire sequence of\u00a0play</h4>\n<p>Before we just connect the data, we have to do a bit of data wrangling to convert the positions of the players for each sequence. This part is going to be pretty code-intensive, but the end result is pretty worth the struggle.</p>\n<pre>positions = []<br>new_cols = columns + ['seq']<br>for seq in data:<br>    for d_seq in data[seq]:<br>        seq_list = d_seq.tolist()<br>        seq_list.append(seq)<br>        positions.append(seq_list)<br>df_data = pd.DataFrame(positions, columns = new_cols)</pre>\n<pre>df_data.seq = df_data.seq.apply(lambda x: int(x.strip('sequence_')))<br>sequences = list(set(df_data.seq))<br>grp = df_data.groupby('seq')<br>sequenced_data = []<br>for sequence in sequences:<br>    sequenced_data.append(grp.get_group(sequence))<br>sequenced_data[0].head(5)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8_QVHqJvb7990IrelzDx0A.png\"><figcaption>We just made a \u2018seq\u2019 column to store the entire dataset within this dataFrame and grouped by\u00a0sequence</figcaption></figure><p>Now more wrangling to combine the X and Y of each player and provide a location on the pitch from the values in the dataFrame above.</p>\n<pre>def parse_data(data, seq):<br>    df = data[seq]<br>    defense = []<br>    attack = []<br>    ball = []<br>    defense.append(df[['GKDef_X','GKDef_Y']].values.tolist())<br>    defense.append(df[['Def1_X','Def1_Y']].values.tolist())<br>    defense.append(df[['Def2_X','Def2_Y']].values.tolist())<br>    defense.append(df[['Def3_X','Def3_Y']].values.tolist())<br>    defense.append(df[['Def4_X','Def4_Y']].values.tolist())<br>    defense.append(df[['Def5_X','Def5_Y']].values.tolist())<br>    defense.append(df[['Def6_X','Def6_Y']].values.tolist())<br>    defense.append(df[['Def7_X','Def7_Y']].values.tolist())<br>    defense.append(df[['Def8_X','Def8_Y']].values.tolist())<br>    defense.append(df[['Def9_X','Def9_Y']].values.tolist())<br>    defense.append(df[['Def10_X','Def10_Y']].values.tolist())<br>    attack.append(df[['GKAtt_X','GKAtt_Y']].values.tolist())<br>    attack.append(df[['Att1_X','Att1_Y']].values.tolist())<br>    attack.append(df[['Att2_X','Att2_Y']].values.tolist())<br>    attack.append(df[['Att3_X','Att3_Y']].values.tolist())<br>    attack.append(df[['Att4_X','Att4_Y']].values.tolist())<br>    attack.append(df[['Att5_X','Att5_Y']].values.tolist())<br>    attack.append(df[['Att6_X','Att6_Y']].values.tolist())<br>    attack.append(df[['Att7_X','Att7_Y']].values.tolist())<br>    attack.append(df[['Att8_X','Att8_Y']].values.tolist())<br>    attack.append(df[['Att9_X','Att9_Y']].values.tolist())<br>    attack.append(df[['Att10_X','Att10_Y']].values.tolist())<br>    ball = df[['Ball_X','Ball_Y']].values.tolist()<br>    def_list = []<br>    att_list = []<br>    for i in range (0,11):<br>        def_list = def_list + defense[i]<br>        att_list = att_list + attack[i]<br><br>    return def_list, att_list, ball</pre>\n<p>Now to the moment of truth, visualizing an entire sequence of play with the defense and attacking players. Here the attacking team is in red and defensive team is in\u00a0blue</p>\n<pre>list_defense, list_attack, list_ball = parse_data(sequenced_data,0)<br>createPitch(list_ball, list_defense, list_attack)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/774/1*wEmKWTo31DYk0ELEZKY_TA.png\"><figcaption>Visualizing a sequence of play, attacking team in red, defending team in\u00a0blue</figcaption></figure><p>Nice! To my trained eye from watching football from a young age, here we are visualizing a defender passing the ball back to the goalkeeper for a long kick towards a striker/midfielder, and the dotted green lines are the\u00a0passes.</p>\n<h3>Statistics for single sequence of\u00a0play</h3>\n<p>I am choosing another sequence to work with here because it has a lot more overlap between the positions of the attacking and defensive team. It looks like a complex sequence of passes leading to a\u00a0goal</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/826/1*YMg3oPLVlxPdWB4bo0q5sQ.png\"><figcaption>The sequence we will be working with for the\u00a0analysis</figcaption></figure><h4>Relationship between players and the\u00a0ball</h4>\n<p>I built a function that finds out the distance of players from the ball and returned a list of distances to plotted the relationship between players and the ball. You could find the code for this part of the analysis on my notebook\u00a0<a href=\"http://http/\">here</a></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*7dWGg66umgQZwvisGhlDAQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*78rqAYhP7nvNDVO6jpXA7Q.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*t4CNfgn2oESqHo3uz8BkMg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*It9SzBrUtVqZ02dHC6fJ9w.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*naDbVz99Ac45OCl0fyJweg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*lU5WegYhLYIbDXqNyrfyYw.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*3PJWOvuMRzn3ODUejrUXzA.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*gb1wruPF4en8GOLQI6NU9w.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*L_ZSjfzOrFoprqBQ9hMIaQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*NHAFnhoVbGfLB3pMY0vmcg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*o7sAMGGrnuk9C7_dEc4R1Q.png\"><figcaption>A complex sequence of play leading to a shot visualized with the player\u2019s distances from the ball plotted over the time of the\u00a0sequence</figcaption></figure><p>We can see how the some players on the attacking and defensive team have rises/lows in the graph at the same time because they are often reactions to the ball being\u00a0passed.</p>\n<h4>Attacking Statistics\u200a\u2014\u200aPossession</h4>\n<p>The possession stats for each team was an easy step from this part of the analysis. Whenever the position of the ball is extremely close to the position of player, the ball is said to be in possession of the player. The sequence should have more possesion for the attacking team rather than the defensive team because the dataset is structured as attacking sequences of\u00a0play.</p>\n<p>Let us take a look at the code for this\u00a0part</p>\n<pre>player_ppa = []<br>player_ppd = []<br>for i in range(0,11):<br>    player_df = pd.DataFrame(player_game_plot[i], columns = ['player', 'seq_num','def_ball','att_ball'])<br>    # Adding values to each team ppa = attacking ppd = defending<br>    player_ppa.append((player_df[player_df['att_ball'] &lt; 2].seq_num.count()/len(player_game_plot[i]))*100)<br>    player_ppd.append((player_df[player_df['def_ball'] &lt; 2].seq_num.count()/len(player_game_plot[i]))*100)</pre>\n<pre># create plot<br>fig, ax = plt.subplots()<br>index = np.arange(0,11)<br>bar_width = 0.4<br>opacity = 0.8<br><br>rects1 = plt.bar(index, player_ppa, bar_width,<br>                 alpha=opacity,<br>                 color='b',<br>                 label='Attacking')<br><br>rects2 = plt.bar(index + bar_width, player_ppd, bar_width,<br>                 alpha=opacity,<br>                 color='g',<br>                 label='Defending')</pre>\n<pre>plt.xlabel(\"Player on team\")<br>plt.ylabel(\"Percentage with Posession\")<br>plt.legend()<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*4HdSQqOvfIs210YkQf1S7w.png\"><figcaption>Possession stats for each player in sequence of\u00a0play</figcaption></figure><p>This a general trend to see attacking team keeps the ball from the defensive team. We can see here that 6 players held the more possession than the others on the attacking team, showing that this attack was the result of 6 players being involved. 6 attacking players were strongly involved with the play and 2 other attacking players were slightly involved in this\u00a0play.</p>\n<h4>Attacking Position\u200a\u2014\u200aDistance from\u00a0goal</h4>\n<p>Now let us take a closer look at the distances from goal for each of the players in this sequence of play. I defined a distance function as square root of (x0-y0)\u00b2 + (x1-y1)\u00b2.</p>\n<pre>def dist(x, y):<br>    return math.sqrt(math.pow((x[0] - y[0]),2) + math.pow((x[1] - y[1]),2))</pre>\n<p>Here I am taking the positions of the attacking players and finding their mean distance from\u00a0goal</p>\n<pre>def attackingPosition(attack):<br>    att_players = np.array_split(attack, 11)<br>    distance = []<br>    # For each sequence time j<br>    for j in range(len(att_players[0])):<br>        # for each player on the attacking team k = 1 to 11<br>        for k in range(11):<br>            # Add the distances of the players on the attacking team to the goal<br>            player_dist = min(dist([52.5,-4], att_players[k][j]),dist([52.5,4], att_players[k][j]))<br>            distance.append((j,k,player_dist))<br><br>    return distance</pre>\n<pre>att_positions = attackingPosition(list_att)<br>att_values = pd.DataFrame(att_positions, columns = ['time', 'player', 'dist'])<br>players = att_values.groupby(['player']).mean()</pre>\n<p>Then I tried to visualize the players\u2019 distances\u2019 from goal and have some fun with colors while doing\u00a0it.</p>\n<pre>def plotDistanceFromGoal(player_distances):<br>    ig, ax = plt.subplots()<br>    index = np.arange(0,11)<br>    bar_width = 0.6<br>    opacity = 0.8</pre>\n<pre>dist = plt.bar(index, player_distances, bar_width,<br>                     alpha=opacity,<br>                     label='Attacking team')<br>    # Some fun with colors<br>    colors = [\"#FF5733\", \"#FFFF33\", \"#33FF39\", \"#33FFE0\", \"#333FFF\", \"#DA33FF\", \"#FF3333\", \"#000000\", \"#0087FF\", \"#B2FF00\", \"#00FFC1\"]<br>    i = 0<br>    for color in colors:<br>        dist[i].set_color(color)<br>        i+=1<br>    plt.xlabel(\"Player on attacking team\")<br>    plt.ylabel(\"Distance from Goal\")<br>    plt.show()<br>plotDistanceFromGoal(players['dist'])</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/806/1*-C2PYo18OvSZKtPsd5KPbg.png\"><figcaption>Distance from goal for each player on the attacking team</figcaption></figure><h4><strong>Attacking Statistics\u200a\u2014\u200aIs it a\u00a0goal?</strong></h4>\n<p>Now one more important thing we can look at is whether the sequence lead to a goal or not. This was surprisingly simple because I could use the last value of sequence and see if it was within the goalposts. Here is the code for\u00a0this.</p>\n<pre>def isGoal(sequence):<br>    seq = sequence[-1]<br>    #print(seq)<br>    if (((-39.5 &lt; seq[0] &lt; -52.5) or (39.5 &lt; seq[0] &lt; 52.5)) and ((-4 &lt; seq[1] &lt; 4))):<br>        return True<br>    else:<br>        return False<br>isGoal(list_ball)</pre>\n<pre># Output: False</pre>\n<p><strong>Ooh, turns out the sequence didn\u2019t result in a goal! </strong>I searched through the dataset for a sequence that did and came up with\u00a0this.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/752/1*ayOpDBFWguVeoWMEbBOyQg.png\"><figcaption>Visualizing a classic counterattack leading to a\u00a0goal</figcaption></figure><p>Wow, a classic counterattack leading to a goal. So our function seems to work in finding a\u00a0goal.</p>\n<h4>Defensive Analysis\u200a\u2014\u200aMarking</h4>\n<p>Now let us take a closer look at the defensive side of the game, here I am defining a new metric as the <strong>closest attacking player </strong>for each player on the defending team to analyze how close the players are marking the players of the attacking team. This is an attempt at feature extraction.</p>\n<p>Let us now look at how good the defensive players are at marking their opponent, here I am assuming that the only metric of marking is the distance from the closest player of the attacking team. It has been pointed out that this assumption may not be right because the player might look to stop the pass of a ball. But for the scope of this analysis I am considering only the distance of the closest attacking player.</p>\n<p>This part of the analysis is to see similarities in play between different players of the same defensive team on how they react to the\u00a0attack.</p>\n<p>In the function below I find the distances of attacking players from each defensive player for each time-step in the sequence.</p>\n<pre>def closestPlayerDist(defense, attack):<br>    def_players = np.array_split(defense, 11)<br>    att_players = np.array_split(attack, 11)<br>    distance = []<br>    # For each sequence time j<br>    for j in range(len(def_players[0])):<br>        # for each player on the defensive team k<br>        for k in range(11):<br>            # Add the distances of the players on the attacking team to the list<br>            player_dist = [dist(def_players[k][j], att_players[i][j]) for i in range(11)]<br>            distance.append((j,k,player_dist))<br><br>    return distance</pre>\n<pre>distances = closestPlayerDist(list_def, list_att)</pre>\n<p>Closest attacking player distance measure will indicate:</p>\n<ul>\n<li>distance themselves how much each player on the defensive team is closing down the space of the attacker they are\u00a0marking</li>\n<li>the mean for each time-step in the sequence would indicate how close the team is to getting the\u00a0ball</li>\n<li>and median for each time-step in the sequence would indicate how close some section of the players to the\u00a0ball</li>\n<li>the variance for each time-step in the sequence in the data shows the \u201cstaticness\u201d of the team, on how much they move to adjust to the teams\u00a0attack</li>\n</ul>\n<pre>def plotdist(distances):<br>    distances_2 = [(distances[i][0], distances[i][1], distances[i][2][np.argmin(distances[i][2])]) for i in range(len(distances))]<br>    x = [distances_2[i][0] for i in range(len(distances_2))]<br>    y = [distances_2[i][2] for i in range(len(distances_2))]<br>    plt.plot(x,y,'bo', alpha=0.2)<br>    plt.ylabel('distance of closest attacking player')<br>    plt.xlabel('time')<br><br>plotdist(distances)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*NL2_z5Sr2-eHWu7rrzA4AA.png\"><figcaption>Distance of closest attacking player for each player on the defending team</figcaption></figure><pre>def plotmeans(distances):<br>    distances_2 = [(distances[i][0], distances[i][1], distances[i][2][np.argmin(distances[i][2])]) for i in range(len(distances))]<br>    values = pd.DataFrame(distances_2, columns = ['time', 'player', 'dist'])<br>    means = values.groupby(['time']).mean()<br>    medians = values.groupby(['time']).median()<br>    fig, ax = plt.subplots()<br>    x = [i for i in range(len(means))]<br>    ax.plot(x,means.dist,'bo', alpha=0.5)<br>    ax.plot(x,medians.dist,'ro', alpha=0.5)<br>    ax.legend(['mean','median'])<br>    plt.ylabel('mean and median distance of closest attacking player')<br>    plt.xlabel('time')<br><br>    plt.show()</pre>\n<pre>plotmeans(distances)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*P4hj23izTaft81w3gD091Q.png\"><figcaption>Mean and median of closest attacking player distance for the sequence of\u00a0play</figcaption></figure><pre>def plotvariance(distances):<br>    distances_2 = [(distances[i][0], distances[i][1], distances[i][2][np.argmin(distances[i][2])]) for i in range(len(distances))]<br>    values = pd.DataFrame(distances_2, columns = ['time', 'player', 'dist'])<br>    variance = values.groupby(['time']).var()<br>    fig, ax = plt.subplots()<br>    x = [i for i in range(len(variance))]<br>    ax.plot(x,variance.dist,'go', alpha=0.2)<br>    ax.legend(['variance'])<br>    plt.ylabel('variance of distance to closest attacking player')<br>    plt.xlabel('time')<br><br>    plt.show()</pre>\n<pre>plotvariance(distances)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*iGqe-fr9iDtBGmdVe40peg.png\"><figcaption>The variance of closest attacking player distance for sequence of\u00a0play</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/296/1*-eBJEjU6VX1FfpP5zk0Vdg.png\"><figcaption>player column indicates defense, att_player indicates attack, time indicates percentage of time in sequence spent marking specific attacking player</figcaption></figure><p>Now let us look at the percentage of time a player of a defensive team marked a specific player of the opposing\u00a0team.</p>\n<p>The code for this\u00a0part</p>\n<pre>distances_3 = [(distances[i][0], distances[i][1], np.argmin(distances[i][2])) for i in range(len(distances))]</pre>\n<pre>marking = pd.DataFrame(distances_3, columns = ['time', 'player', 'att_player'])<br>marking = marking.groupby(['player','att_player']).count().sort_values(by='time', ascending=False)<br>marking.time = (marking.time/(len(distances)/11))*100<br>marking</pre>\n<p>To my trained eye, we can see defensive players 2, 5, 10, 6, and 1(goalkeeper) were close to their attacking player but there is a sharp drop after that which indicates that the defensive players were either passed by an attacking player or were playing a zonal defense as a\u00a0team.</p>\n<p>I don\u2019t think statistically it is a great feature otherwise.</p>\n<h3>Visualizing positions of players on the pitch throughout the\u00a0game,</h3>\n<p>Since we have now looked at a single sequence of play and analyzed it, let us look at the dataset as a\u00a0whole.</p>\n<h4>Heat map of players positions</h4>\n<p>Here we are going to build a heatmap of the player\u2019s positions and display the positions taken up by each of the\u00a0players</p>\n<p>I first built a helper function to parse the data and build a heat map of players positions. Next, I had to parse the values as integer because I was using plt.pcolor and it had to have integer values in the data. 52 and 35 were hard constants to make sure I did not have any negative values in my\u00a0data.</p>\n<pre>def parse_sequence(values, data):<br>    round_data = [(round(x[0]),round(x[1])) for x in data]<br>    for r in round_data:<br>        a = r[0] + 52<br>        b = r[1] + 35<br>        values[b,a] += 1</pre>\n<p>Here I obtained the raw data and extracted the values of player positions from the data, populating the grid with positions of\u00a0players</p>\n<pre>values = np.zeros((71,107))<br>ball = np.zeros((71,107))<br>for i in range(len(sequenced_data)):<br>    list_defense, list_attack, list_ball = parse_data(sequenced_data,i)<br>    parse_sequence(values, list_defense)<br>    parse_sequence(values, list_attack)<br>    parse_sequence(ball, list_ball)</pre>\n<p>Finally, for the second moment of truth in this analysis, heatmap for the positions of the\u00a0players</p>\n<pre>def createGrid(values, title_str):<br>    v = np.asarray(values)<br>    plt.pcolor(v)<br>    plt.title(title_str)<br>    plt.legend()<br>    plt.show()<br>createGrid(values, \"Heatmap for player's position on pitch\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5zOiV8B_jeru-XUUQCAVXA.png\"><figcaption>Look at this thing of beauty, heat map for positions of players for all the sequences of\u00a0play</figcaption></figure><p>Now moving on, I would like to see the passes made in the sequences, and the heatmap for position of the ball throughout the\u00a0game.</p>\n<pre>createGrid(ball, \"Heatmap for ball position on pitch\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SQN0_VVObAgOKUjpYphzjw.png\"><figcaption>Heat map for position of the ball throughout the entire\u00a0game</figcaption></figure><h3>KMeans clustering analysis</h3>\n<p>Clearly having a set of points that I could cluster, I released that KMeans is the easiest way to cluster this data to show some interesting insights into the\u00a0data</p>\n<p><strong>Reasons why I chose\u00a0KMeans</strong></p>\n<ul>\n<li>KMeans clustering is used to classify unstructured data\u200a\u2014\u200ahaving unstructured data with two features, the X and Y coordinate of each player\u2019s position suited well for the\u00a0analysis</li>\n<li>KMeans works by finding the centroids of the feature mappings of the data\u200a\u2014\u200aI am using it to find the centroid of the X,Y coordinates of the player\u2019s positions, this provides an insight into the relative structure of the team for the attacking and defensive team</li>\n<li>Using KMeans on positions of both defensive and attacking players would shed light on the natural positions of the pitch in which the game is contested in</li>\n</ul>\n<p>Lets get down to the code, this is for obtaining all the data to feed into our KMeans classifier</p>\n<pre>from sklearn.cluster import KMeans</pre>\n<pre>list_def = []<br>list_att = []<br>list_bll = []<br>for i in range(len(sequenced_data)):<br>    list_defense, list_attack, list_ball = parse_data(sequenced_data,i)<br>    list_def = list_def + list_defense<br>    list_att = list_att + list_attack<br>    list_bll = list_bll + list_ball</pre>\n<p>Now over to the KMeans classifier and visualizing code. The notebook is also available to over\u00a0<a href=\"https://github.com/raghav96/datascience/blob/master/Football%20Analysis.ipynb\">here</a>.</p>\n<pre>def plotKMeans(train, data, title_str, k=11):<br>    # Number of clusters<br>    kmeans = KMeans(n_clusters=k)<br>    # Fitting the input data<br>    kmeans = kmeans.fit(train)<br>    # Getting the cluster labels<br>    labels = kmeans.predict(data)<br>    # Centroid values<br>    centroids = kmeans.cluster_centers_<br>    def xy(lst):<br>        x = [l[0] for l in lst]<br>        y = [l[1] for l in lst]<br>        return x, y<br>    x, y = xy(data)<br>    fig, ax = plt.subplots()<br>    if (title_str[0] == 'D'):<br>        ax.scatter(x, y, marker='o', color='#ffff00', alpha=0.03)<br>    else:<br>        ax.scatter(x, y, marker='o', color='#00ffff', alpha=0.03)</pre>\n<pre>ax.scatter(centroids[:,0], y=centroids[:,1], marker='*', c='#050505', s=100)<br>    plt.title(title_str)<br>    plt.legend()<br>    plt.show()<br>    return centroids</pre>\n<h4>KMeans centroids for defense and attacking team</h4>\n<p>And now plotting the KMeans centroids and positions of the defensive team</p>\n<pre>def_average_pos = plotKMeans(list_def, list_def, \"Defensive team average positions\")<br>att_average_pos = plotKMeans(list_att, list_att, \"Attacking team average positions\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*klx_PpFtQD2uXeHpQq-Utw.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*rBV4rNY9NFXK9T_fuf3jZw.png\"><figcaption>These do indeed look like formations, but we\u2019ll come to that in a\u00a0second.</figcaption></figure><h4>Relationship to\u00a0ball</h4>\n<p>So this time, I tried to see how the positions of the ball might affect the KMeans clusters of both the attacking and defensive team, but it doesn\u2019t seem to have much\u00a0effect</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*2tr-T29N_SQpysm5X6glyQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*8PhHvAypoTvypHH8syxTjw.png\"><figcaption>Not much of a difference eh?</figcaption></figure><h4>Areas of pitch with most\u00a0activity</h4>\n<p>Now let us find out which parts of the pitch were the most occupied, an insight to see the nature of the game at large. Here I used the number of clusters to be 10 for the entire dataset, hoping to see 10 centroids on the pitch players of both teams are occupying. Here is the code for\u00a0this</p>\n<pre>plotKMeans(list_def+list_att, list_def+list_att, \"Areas of the pitch with most activity\", 10)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*xKS_0tVhs6btU7VuM4kvzQ.png\"><figcaption>This resulted in this heat map showing 10 positions (KMeans centroids) on the pitch that players of both teams\u00a0occupy</figcaption></figure><h4>Formations</h4>\n<p>Now finally coming to the obvious here, we can see on using KMeans clustering with 11 clusters, we could find the positioning of each player of the defending and attacking team, creating a formation. Now I took the centroids from the defending and attacking teams\u2019 positions and plotted them, they came up looking just like a formation. Putting this on the football pitch from the earlier part of this analysis put it really into\u00a0context.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*n8aaa8P3yk_jtRtPEEPW3A.png\"><figcaption>My first try to plot the centroids of the attacking and defending team</figcaption></figure><p>Nice! To my trained eye, I can see a defense lining up as 3\u20133\u20133\u20131 formation vs attack lining up as 4\u20132\u20133\u20131 formation with the full-backs pushed up higher, let me display that on a\u00a0pitch</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*7y9vI24f9YHDr-kOkHfMwg.png\"><figcaption>The attack is red and defense is\u00a0blue.</figcaption></figure><h3>The End</h3>\n<p>There we go, from the positions of players on the pitch, we have analyzed sequences of play for different metrics, how the players moved with respect to the ball, with respect to the opposing team, tried to build a feature, analyzed positions occupied on the pitch, tried to classify the data using KMeans clustering and ended up with this visualization of formations using KMeans centroids</p>\n<p>It was a challenging dataset to work with, but we came up with some interesting results at the end. I look forward to your comments on this <a href=\"https://github.com/raghav96/datascience/blob/master/Football%20Analysis.ipynb\">notebook</a> and suggest more\u00a0tips.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f903d2c9e2ff\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/the-football-derby-a-data-science-story-f903d2c9e2ff\">The Football Derby \u2014 A Data Science Story</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<p>So this is my next article in a series of Medium posts this summer where I have worked on building my skills in data science to work towards a career as a data scientist.</p>\n<p>Working with football data is something that I wanted to do for a long time and this time, I have taken the opportunity to build an analysis of different sequences of play and the game at large using a dataset with positions of soccer players to come up with some interesting insights about the sequence of play or the game\u00a0itself.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*uEYgYwJmH9-05O71\"><figcaption>Photo by <a href=\"https://unsplash.com/@willianjusten?utm_source=medium&amp;utm_medium=referral\">Willian Justen de Vasconcellos</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>I acquired this dataset from STATS, from their website <a href=\"https://www.stats.com/data-science/\">https://www.stats.com/data-science/</a>. You can fill out the request form for the soccer data from their website in order to send a request for the data, after which they would send you an agreement to sign before providing the dataset to\u00a0you.</p>\n<p>The Jupyter notebook for this analysis is\u00a0<a href=\"https://www.github.com/raghav96/datascience\">here</a>.</p>\n<h3>Starting off</h3>\n<p>So to sum the things that I intend to do in this\u00a0analysis</p>\n<ul>\n<li>visualize sequences of play in football,</li>\n<li>display statistics about sequence with respect to a single sequence of\u00a0play,</li>\n<li>visualize positions of players on the pitch throughout the\u00a0game,</li>\n<li>Apply KMeans clustering on the dataset in order to come up with interesting insights about the\u00a0data</li>\n</ul>\n<h3>Dataset Structure</h3>\n<p>The dataset contains multiple columns of sequences (\u2018sequence_1\u2019 to \u2018sequence_18\u2019) of play from actual football games in which each players and the balls positions are\u00a0recorded</p>\n<p>Let us define the columns in the data based on available information above: first 22 columns are X,Y positions of defensive players, second 22 columns are X,Y columns of offensive players, last 2 columns are X,Y position of the ball in\u00a0play</p>\n<p>TLDR; This information about the dataset came with the dataset, read this if you would like detailed information about the\u00a0dataset</p>\n<blockquote>Included in the data set are a training set with 7500 sequences and two separate set of sequences for testing. Python users can load the training data (in pickle format) into a dictionary, the key for each sequence would be \u201c\u200b<strong><em>sequence_n</em></strong>\u200b\u201d,where n is anywhere from 1 to\u00a07500.</blockquote>\n<blockquote>Each sequence contains a segment of tracking data corresponding to actual game play from a recent professional soccer league. The format of each sequence is as\u00a0follows:</blockquote>\n<blockquote>- Each sequence is a matrix (numpy 2D array) with 46 columns. Each row contains 23 pairs of (x,y) coordinates of 22 players from both teams and the ball at frequency of\u00a010Hz.</blockquote>\n<blockquote>- In the first 22 columns are 11 (x,y) pairs of defense team. The following 22 columns are coordinates of attacking team (defined as the team with consecutive possession of the ball). The last 2 columns are coordinates of the\u00a0ball.</blockquote>\n<blockquote>- Each set of 22 columns for both attacking and defending team consist of (x,y) pair for the goalkeeper, followed by 10 consecutive (x,y) pairs for the other 10 teammates. The identities and teams vary from sequence to sequence. However, within each sequence, the identity is consistent. Thus concretely, out of the 46 columns from each sequence, we know that the first 2 columns represent the coordinate of defense team\u2019s keeper. Columns 2 to 22 contain 10 consecutive (x,y) pairs of other defensive players. Columns 23 and 24 carry x and y coordinates of the attacking team\u2019s keeper. Columns 25 to 44 contain 10 consecutive (x,y) pairs of other attacking players. Columns 45 and 46 carry x and y coordinates of the\u00a0ball.</blockquote>\n<blockquote>- The coordinates generally belong to the [-52.5 meter, +52.5 meter] range along the x-axis, and [-34 meter, +34 meter] range along the y-axis, with the very center of the pitch being [0,0]. So for example, to normalize the data to the range [-1,+1], one can simply divide the x-columns by 52.5 and y-columns by 34 (this effectively will re-scale the pitch, which roughly corresponds to soccer field of size 105mx70m, from a rectangular box to a square\u00a0box)</blockquote>\n<blockquote>- The coordinates were also adjusted so that the attacking team will moves from left to right, meaning the defending team defends the goal on the right hand\u00a0side.</blockquote>\n<blockquote>- In aggregate, the data set amounts to equivalently and approximately 45 games worth of playing time, with redundant and \u201cdead\u201d situations removed.</blockquote>\n<h3>Acquiring and Wranging\u00a0Data</h3>\n<p>To start off this analysis I imported the dataset into the Jupyter notebook. The data was in the \u2018.pkl\u2019 format, which was new to me, however a quick Google search revealed the <em>pd.read_pickle</em> function in\u00a0pandas.</p>\n<pre>import pandas as pd<br>import seaborn as sns<br>import matplotlib.pyplot as plt<br>from matplotlib import patches<br>import numpy as np<br>%matplotlib inline</pre>\n<pre>data = pd.read_pickle('football/test_data_1.pkl')</pre>\n<p>Let us define the columns in the data based on available information above: first 22 columns are X,Y positions of defensive players, second 22 columns are X,Y columns of offensive players, last 2 columns are X,Y position of the ball in\u00a0play</p>\n<pre>columns = ['GKDef_X', 'GKDef_Y', 'Def1_X', 'Def1_Y','Def2_X', 'Def2_Y','Def3_X', 'Def3_Y','Def4_X', 'Def4_Y','Def5_X', 'Def5_Y', 'Def6_X', 'Def6_Y', 'Def7_X', 'Def7_Y','Def8_X', 'Def8_Y','Def9_X', 'Def9_Y','Def10_X', 'Def10_Y', 'GKAtt_X','GKAtt_Y','Att1_X','Att1_Y','Att2_X','Att2_Y','Att3_X','Att3_Y','Att4_X','Att4_Y','Att5_X','Att5_Y','Att6_X','Att6_Y','Att7_X','Att7_Y','Att8_X','Att8_Y','Att9_X','Att9_Y','Att10_X','Att10_Y','Ball_X','Ball_Y']</pre>\n<p>From here, its now time to make a pandas DataFrame to hold the\u00a0values</p>\n<pre>test_sequence = 'sequence_1'<br>df = pd.DataFrame(data[test_sequence], columns = columns)<br>df.head(5)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0Zxuxm78gJtbtJZQ37e4XQ.png\"><figcaption>Result from the code above, we can see now the data split into the respective columns in a DataFrame</figcaption></figure><h4>Visualize football pitch and movement of the\u00a0ball</h4>\n<p>In this part of the analysis, we are going to convert what we have above into a visualization of the position of the ball. Let us the use the columns we designed above to convert the data into a DataFrame and display the trajectory of the ball for a test sequence. Here is code for ball position for a single point of time in a sequence.</p>\n<pre># Obtaining the positions occupied by the ball<br>ball_pos = df[['Ball_X','Ball_Y']].values.tolist()<br>ball_pos[0]</pre>\n<pre># Output = [-19.961977005004883, 3.843478202819824]</pre>\n<p>Here is the code for building the pitch, here I used matplotlib to build it out, and used code from this\u00a0<a href=\"https://fcpython.com/visualisation/drawing-pitchmap-adding-lines-circles-matplotlib\">article</a>.</p>\n<pre># Plotting the ball along with the position of the ball<br># Run with createPitch([]) to just display the pitch<br>def createPitch(ball_pos, defense_pos, attack_pos, size=0.2):<br>    #Create figure<br>    fig=plt.figure()<br>    ax=fig.add_subplot(1,1,1)</pre>\n<pre>#Pitch Outline &amp; Centre Line<br>    plt.plot([-52.5,-52.5],[-35,35], color=\"black\")<br>    plt.plot([-52.5,52.5],[35,35], color=\"black\")<br>    plt.plot([52.5,52.5],[35,-35], color=\"black\")<br>    plt.plot([52.5,-52.5],[-35,-35], color=\"black\")<br>    plt.plot([0,0],[-35,35], color=\"black\")<br><br>    #Left Penalty Area<br>    plt.plot([-39.5,-39.5],[16,-16],color=\"black\")<br>    plt.plot([-52.5,-39.5],[16,16],color=\"black\")<br>    plt.plot([-39.5,-52.5],[-16,-16],color=\"black\")<br><br>    #Right Penalty Area<br>    plt.plot([39.5,39.5],[16,-16],color=\"black\")<br>    plt.plot([52.5,39.5],[16,16],color=\"black\")<br>    plt.plot([39.5,52.5],[-16,-16],color=\"black\")<br><br>    #Left 6-yard Box<br>    plt.plot([-52.5,-47.5],[-10,-10],color=\"black\")<br>    plt.plot([-47.5,-47.5],[-10,10],color=\"black\")<br>    plt.plot([-52.5,-47.5],[10,10],color=\"black\")<br><br>    #Right 6-yard Box<br>    plt.plot([52.5,47.5],[-10,-10],color=\"black\")<br>    plt.plot([47.5,47.5],[-10,10],color=\"black\")<br>    plt.plot([52.5,47.5],[10,10],color=\"black\")<br><br>    #Create Arc and add it to our plot<br>    leftArc = patches.Arc((-45.5,0),height=18.3,width=18.3,angle=0,theta1=310,theta2=50,color=\"black\")<br>    rightArc = patches.Arc((45.5,0),height=18.3,width=18.3,angle=0,theta1=130,theta2=230,color=\"black\")</pre>\n<pre>#Assign circles to variables - do not fill the centre circle!<br>    centreCircle = plt.Circle((0,0),10,color=\"black\",fill=False)<br>    centreSpot = plt.Circle((0,0),0.8,color=\"black\")<br>    leftPenSpot = plt.Circle((-43.5,0),0.4, color=\"black\")<br>    rightPenSpot = plt.Circle((43.5,0),0.4, color=\"black\")<br><br>    #Draw Arcs<br>    ax.add_patch(leftArc)<br>    ax.add_patch(rightArc)</pre>\n<pre>#Draw the circles to our plot<br>    ax.add_patch(centreCircle)<br>    ax.add_patch(centreSpot)<br>    ax.add_patch(leftPenSpot)<br>    ax.add_patch(rightPenSpot)<br><br>    # Plotting the ball<br>    for pos in attack_pos:<br>        locBall = plt.Circle((pos[0],pos[1]),size,color=\"red\")<br>        ax.add_patch(locBall)<br>    for pos in defense_pos:<br>        locBall = plt.Circle((pos[0],pos[1]),size,color=\"blue\")<br>        ax.add_patch(locBall)<br>    for pos in ball_pos:<br>        locBall = plt.Circle((pos[0],pos[1]),size,color=\"green\")<br>        ax.add_patch(locBall)<br><br><br>createPitch(ball_pos, [], [])</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/752/1*AAZhd8FFVRVWyMPiSUnapA.png\"><figcaption>Result from the code above, position of the\u00a0ball</figcaption></figure><h4>Visualizing an entire sequence of\u00a0play</h4>\n<p>Before we just connect the data, we have to do a bit of data wrangling to convert the positions of the players for each sequence. This part is going to be pretty code-intensive, but the end result is pretty worth the struggle.</p>\n<pre>positions = []<br>new_cols = columns + ['seq']<br>for seq in data:<br>    for d_seq in data[seq]:<br>        seq_list = d_seq.tolist()<br>        seq_list.append(seq)<br>        positions.append(seq_list)<br>df_data = pd.DataFrame(positions, columns = new_cols)</pre>\n<pre>df_data.seq = df_data.seq.apply(lambda x: int(x.strip('sequence_')))<br>sequences = list(set(df_data.seq))<br>grp = df_data.groupby('seq')<br>sequenced_data = []<br>for sequence in sequences:<br>    sequenced_data.append(grp.get_group(sequence))<br>sequenced_data[0].head(5)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8_QVHqJvb7990IrelzDx0A.png\"><figcaption>We just made a \u2018seq\u2019 column to store the entire dataset within this dataFrame and grouped by\u00a0sequence</figcaption></figure><p>Now more wrangling to combine the X and Y of each player and provide a location on the pitch from the values in the dataFrame above.</p>\n<pre>def parse_data(data, seq):<br>    df = data[seq]<br>    defense = []<br>    attack = []<br>    ball = []<br>    defense.append(df[['GKDef_X','GKDef_Y']].values.tolist())<br>    defense.append(df[['Def1_X','Def1_Y']].values.tolist())<br>    defense.append(df[['Def2_X','Def2_Y']].values.tolist())<br>    defense.append(df[['Def3_X','Def3_Y']].values.tolist())<br>    defense.append(df[['Def4_X','Def4_Y']].values.tolist())<br>    defense.append(df[['Def5_X','Def5_Y']].values.tolist())<br>    defense.append(df[['Def6_X','Def6_Y']].values.tolist())<br>    defense.append(df[['Def7_X','Def7_Y']].values.tolist())<br>    defense.append(df[['Def8_X','Def8_Y']].values.tolist())<br>    defense.append(df[['Def9_X','Def9_Y']].values.tolist())<br>    defense.append(df[['Def10_X','Def10_Y']].values.tolist())<br>    attack.append(df[['GKAtt_X','GKAtt_Y']].values.tolist())<br>    attack.append(df[['Att1_X','Att1_Y']].values.tolist())<br>    attack.append(df[['Att2_X','Att2_Y']].values.tolist())<br>    attack.append(df[['Att3_X','Att3_Y']].values.tolist())<br>    attack.append(df[['Att4_X','Att4_Y']].values.tolist())<br>    attack.append(df[['Att5_X','Att5_Y']].values.tolist())<br>    attack.append(df[['Att6_X','Att6_Y']].values.tolist())<br>    attack.append(df[['Att7_X','Att7_Y']].values.tolist())<br>    attack.append(df[['Att8_X','Att8_Y']].values.tolist())<br>    attack.append(df[['Att9_X','Att9_Y']].values.tolist())<br>    attack.append(df[['Att10_X','Att10_Y']].values.tolist())<br>    ball = df[['Ball_X','Ball_Y']].values.tolist()<br>    def_list = []<br>    att_list = []<br>    for i in range (0,11):<br>        def_list = def_list + defense[i]<br>        att_list = att_list + attack[i]<br><br>    return def_list, att_list, ball</pre>\n<p>Now to the moment of truth, visualizing an entire sequence of play with the defense and attacking players. Here the attacking team is in red and defensive team is in\u00a0blue</p>\n<pre>list_defense, list_attack, list_ball = parse_data(sequenced_data,0)<br>createPitch(list_ball, list_defense, list_attack)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/774/1*wEmKWTo31DYk0ELEZKY_TA.png\"><figcaption>Visualizing a sequence of play, attacking team in red, defending team in\u00a0blue</figcaption></figure><p>Nice! To my trained eye from watching football from a young age, here we are visualizing a defender passing the ball back to the goalkeeper for a long kick towards a striker/midfielder, and the dotted green lines are the\u00a0passes.</p>\n<h3>Statistics for single sequence of\u00a0play</h3>\n<p>I am choosing another sequence to work with here because it has a lot more overlap between the positions of the attacking and defensive team. It looks like a complex sequence of passes leading to a\u00a0goal</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/826/1*YMg3oPLVlxPdWB4bo0q5sQ.png\"><figcaption>The sequence we will be working with for the\u00a0analysis</figcaption></figure><h4>Relationship between players and the\u00a0ball</h4>\n<p>I built a function that finds out the distance of players from the ball and returned a list of distances to plotted the relationship between players and the ball. You could find the code for this part of the analysis on my notebook\u00a0<a href=\"http://http/\">here</a></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*7dWGg66umgQZwvisGhlDAQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*78rqAYhP7nvNDVO6jpXA7Q.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*t4CNfgn2oESqHo3uz8BkMg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*It9SzBrUtVqZ02dHC6fJ9w.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*naDbVz99Ac45OCl0fyJweg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*lU5WegYhLYIbDXqNyrfyYw.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*3PJWOvuMRzn3ODUejrUXzA.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*gb1wruPF4en8GOLQI6NU9w.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*L_ZSjfzOrFoprqBQ9hMIaQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*NHAFnhoVbGfLB3pMY0vmcg.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*o7sAMGGrnuk9C7_dEc4R1Q.png\"><figcaption>A complex sequence of play leading to a shot visualized with the player\u2019s distances from the ball plotted over the time of the\u00a0sequence</figcaption></figure><p>We can see how the some players on the attacking and defensive team have rises/lows in the graph at the same time because they are often reactions to the ball being\u00a0passed.</p>\n<h4>Attacking Statistics\u200a\u2014\u200aPossession</h4>\n<p>The possession stats for each team was an easy step from this part of the analysis. Whenever the position of the ball is extremely close to the position of player, the ball is said to be in possession of the player. The sequence should have more possesion for the attacking team rather than the defensive team because the dataset is structured as attacking sequences of\u00a0play.</p>\n<p>Let us take a look at the code for this\u00a0part</p>\n<pre>player_ppa = []<br>player_ppd = []<br>for i in range(0,11):<br>    player_df = pd.DataFrame(player_game_plot[i], columns = ['player', 'seq_num','def_ball','att_ball'])<br>    # Adding values to each team ppa = attacking ppd = defending<br>    player_ppa.append((player_df[player_df['att_ball'] &lt; 2].seq_num.count()/len(player_game_plot[i]))*100)<br>    player_ppd.append((player_df[player_df['def_ball'] &lt; 2].seq_num.count()/len(player_game_plot[i]))*100)</pre>\n<pre># create plot<br>fig, ax = plt.subplots()<br>index = np.arange(0,11)<br>bar_width = 0.4<br>opacity = 0.8<br><br>rects1 = plt.bar(index, player_ppa, bar_width,<br>                 alpha=opacity,<br>                 color='b',<br>                 label='Attacking')<br><br>rects2 = plt.bar(index + bar_width, player_ppd, bar_width,<br>                 alpha=opacity,<br>                 color='g',<br>                 label='Defending')</pre>\n<pre>plt.xlabel(\"Player on team\")<br>plt.ylabel(\"Percentage with Posession\")<br>plt.legend()<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*4HdSQqOvfIs210YkQf1S7w.png\"><figcaption>Possession stats for each player in sequence of\u00a0play</figcaption></figure><p>This a general trend to see attacking team keeps the ball from the defensive team. We can see here that 6 players held the more possession than the others on the attacking team, showing that this attack was the result of 6 players being involved. 6 attacking players were strongly involved with the play and 2 other attacking players were slightly involved in this\u00a0play.</p>\n<h4>Attacking Position\u200a\u2014\u200aDistance from\u00a0goal</h4>\n<p>Now let us take a closer look at the distances from goal for each of the players in this sequence of play. I defined a distance function as square root of (x0-y0)\u00b2 + (x1-y1)\u00b2.</p>\n<pre>def dist(x, y):<br>    return math.sqrt(math.pow((x[0] - y[0]),2) + math.pow((x[1] - y[1]),2))</pre>\n<p>Here I am taking the positions of the attacking players and finding their mean distance from\u00a0goal</p>\n<pre>def attackingPosition(attack):<br>    att_players = np.array_split(attack, 11)<br>    distance = []<br>    # For each sequence time j<br>    for j in range(len(att_players[0])):<br>        # for each player on the attacking team k = 1 to 11<br>        for k in range(11):<br>            # Add the distances of the players on the attacking team to the goal<br>            player_dist = min(dist([52.5,-4], att_players[k][j]),dist([52.5,4], att_players[k][j]))<br>            distance.append((j,k,player_dist))<br><br>    return distance</pre>\n<pre>att_positions = attackingPosition(list_att)<br>att_values = pd.DataFrame(att_positions, columns = ['time', 'player', 'dist'])<br>players = att_values.groupby(['player']).mean()</pre>\n<p>Then I tried to visualize the players\u2019 distances\u2019 from goal and have some fun with colors while doing\u00a0it.</p>\n<pre>def plotDistanceFromGoal(player_distances):<br>    ig, ax = plt.subplots()<br>    index = np.arange(0,11)<br>    bar_width = 0.6<br>    opacity = 0.8</pre>\n<pre>dist = plt.bar(index, player_distances, bar_width,<br>                     alpha=opacity,<br>                     label='Attacking team')<br>    # Some fun with colors<br>    colors = [\"#FF5733\", \"#FFFF33\", \"#33FF39\", \"#33FFE0\", \"#333FFF\", \"#DA33FF\", \"#FF3333\", \"#000000\", \"#0087FF\", \"#B2FF00\", \"#00FFC1\"]<br>    i = 0<br>    for color in colors:<br>        dist[i].set_color(color)<br>        i+=1<br>    plt.xlabel(\"Player on attacking team\")<br>    plt.ylabel(\"Distance from Goal\")<br>    plt.show()<br>plotDistanceFromGoal(players['dist'])</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/806/1*-C2PYo18OvSZKtPsd5KPbg.png\"><figcaption>Distance from goal for each player on the attacking team</figcaption></figure><h4><strong>Attacking Statistics\u200a\u2014\u200aIs it a\u00a0goal?</strong></h4>\n<p>Now one more important thing we can look at is whether the sequence lead to a goal or not. This was surprisingly simple because I could use the last value of sequence and see if it was within the goalposts. Here is the code for\u00a0this.</p>\n<pre>def isGoal(sequence):<br>    seq = sequence[-1]<br>    #print(seq)<br>    if (((-39.5 &lt; seq[0] &lt; -52.5) or (39.5 &lt; seq[0] &lt; 52.5)) and ((-4 &lt; seq[1] &lt; 4))):<br>        return True<br>    else:<br>        return False<br>isGoal(list_ball)</pre>\n<pre># Output: False</pre>\n<p><strong>Ooh, turns out the sequence didn\u2019t result in a goal! </strong>I searched through the dataset for a sequence that did and came up with\u00a0this.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/752/1*ayOpDBFWguVeoWMEbBOyQg.png\"><figcaption>Visualizing a classic counterattack leading to a\u00a0goal</figcaption></figure><p>Wow, a classic counterattack leading to a goal. So our function seems to work in finding a\u00a0goal.</p>\n<h4>Defensive Analysis\u200a\u2014\u200aMarking</h4>\n<p>Now let us take a closer look at the defensive side of the game, here I am defining a new metric as the <strong>closest attacking player </strong>for each player on the defending team to analyze how close the players are marking the players of the attacking team. This is an attempt at feature extraction.</p>\n<p>Let us now look at how good the defensive players are at marking their opponent, here I am assuming that the only metric of marking is the distance from the closest player of the attacking team. It has been pointed out that this assumption may not be right because the player might look to stop the pass of a ball. But for the scope of this analysis I am considering only the distance of the closest attacking player.</p>\n<p>This part of the analysis is to see similarities in play between different players of the same defensive team on how they react to the\u00a0attack.</p>\n<p>In the function below I find the distances of attacking players from each defensive player for each time-step in the sequence.</p>\n<pre>def closestPlayerDist(defense, attack):<br>    def_players = np.array_split(defense, 11)<br>    att_players = np.array_split(attack, 11)<br>    distance = []<br>    # For each sequence time j<br>    for j in range(len(def_players[0])):<br>        # for each player on the defensive team k<br>        for k in range(11):<br>            # Add the distances of the players on the attacking team to the list<br>            player_dist = [dist(def_players[k][j], att_players[i][j]) for i in range(11)]<br>            distance.append((j,k,player_dist))<br><br>    return distance</pre>\n<pre>distances = closestPlayerDist(list_def, list_att)</pre>\n<p>Closest attacking player distance measure will indicate:</p>\n<ul>\n<li>distance themselves how much each player on the defensive team is closing down the space of the attacker they are\u00a0marking</li>\n<li>the mean for each time-step in the sequence would indicate how close the team is to getting the\u00a0ball</li>\n<li>and median for each time-step in the sequence would indicate how close some section of the players to the\u00a0ball</li>\n<li>the variance for each time-step in the sequence in the data shows the \u201cstaticness\u201d of the team, on how much they move to adjust to the teams\u00a0attack</li>\n</ul>\n<pre>def plotdist(distances):<br>    distances_2 = [(distances[i][0], distances[i][1], distances[i][2][np.argmin(distances[i][2])]) for i in range(len(distances))]<br>    x = [distances_2[i][0] for i in range(len(distances_2))]<br>    y = [distances_2[i][2] for i in range(len(distances_2))]<br>    plt.plot(x,y,'bo', alpha=0.2)<br>    plt.ylabel('distance of closest attacking player')<br>    plt.xlabel('time')<br><br>plotdist(distances)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*NL2_z5Sr2-eHWu7rrzA4AA.png\"><figcaption>Distance of closest attacking player for each player on the defending team</figcaption></figure><pre>def plotmeans(distances):<br>    distances_2 = [(distances[i][0], distances[i][1], distances[i][2][np.argmin(distances[i][2])]) for i in range(len(distances))]<br>    values = pd.DataFrame(distances_2, columns = ['time', 'player', 'dist'])<br>    means = values.groupby(['time']).mean()<br>    medians = values.groupby(['time']).median()<br>    fig, ax = plt.subplots()<br>    x = [i for i in range(len(means))]<br>    ax.plot(x,means.dist,'bo', alpha=0.5)<br>    ax.plot(x,medians.dist,'ro', alpha=0.5)<br>    ax.legend(['mean','median'])<br>    plt.ylabel('mean and median distance of closest attacking player')<br>    plt.xlabel('time')<br><br>    plt.show()</pre>\n<pre>plotmeans(distances)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*P4hj23izTaft81w3gD091Q.png\"><figcaption>Mean and median of closest attacking player distance for the sequence of\u00a0play</figcaption></figure><pre>def plotvariance(distances):<br>    distances_2 = [(distances[i][0], distances[i][1], distances[i][2][np.argmin(distances[i][2])]) for i in range(len(distances))]<br>    values = pd.DataFrame(distances_2, columns = ['time', 'player', 'dist'])<br>    variance = values.groupby(['time']).var()<br>    fig, ax = plt.subplots()<br>    x = [i for i in range(len(variance))]<br>    ax.plot(x,variance.dist,'go', alpha=0.2)<br>    ax.legend(['variance'])<br>    plt.ylabel('variance of distance to closest attacking player')<br>    plt.xlabel('time')<br><br>    plt.show()</pre>\n<pre>plotvariance(distances)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/386/1*iGqe-fr9iDtBGmdVe40peg.png\"><figcaption>The variance of closest attacking player distance for sequence of\u00a0play</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/296/1*-eBJEjU6VX1FfpP5zk0Vdg.png\"><figcaption>player column indicates defense, att_player indicates attack, time indicates percentage of time in sequence spent marking specific attacking player</figcaption></figure><p>Now let us look at the percentage of time a player of a defensive team marked a specific player of the opposing\u00a0team.</p>\n<p>The code for this\u00a0part</p>\n<pre>distances_3 = [(distances[i][0], distances[i][1], np.argmin(distances[i][2])) for i in range(len(distances))]</pre>\n<pre>marking = pd.DataFrame(distances_3, columns = ['time', 'player', 'att_player'])<br>marking = marking.groupby(['player','att_player']).count().sort_values(by='time', ascending=False)<br>marking.time = (marking.time/(len(distances)/11))*100<br>marking</pre>\n<p>To my trained eye, we can see defensive players 2, 5, 10, 6, and 1(goalkeeper) were close to their attacking player but there is a sharp drop after that which indicates that the defensive players were either passed by an attacking player or were playing a zonal defense as a\u00a0team.</p>\n<p>I don\u2019t think statistically it is a great feature otherwise.</p>\n<h3>Visualizing positions of players on the pitch throughout the\u00a0game,</h3>\n<p>Since we have now looked at a single sequence of play and analyzed it, let us look at the dataset as a\u00a0whole.</p>\n<h4>Heat map of players positions</h4>\n<p>Here we are going to build a heatmap of the player\u2019s positions and display the positions taken up by each of the\u00a0players</p>\n<p>I first built a helper function to parse the data and build a heat map of players positions. Next, I had to parse the values as integer because I was using plt.pcolor and it had to have integer values in the data. 52 and 35 were hard constants to make sure I did not have any negative values in my\u00a0data.</p>\n<pre>def parse_sequence(values, data):<br>    round_data = [(round(x[0]),round(x[1])) for x in data]<br>    for r in round_data:<br>        a = r[0] + 52<br>        b = r[1] + 35<br>        values[b,a] += 1</pre>\n<p>Here I obtained the raw data and extracted the values of player positions from the data, populating the grid with positions of\u00a0players</p>\n<pre>values = np.zeros((71,107))<br>ball = np.zeros((71,107))<br>for i in range(len(sequenced_data)):<br>    list_defense, list_attack, list_ball = parse_data(sequenced_data,i)<br>    parse_sequence(values, list_defense)<br>    parse_sequence(values, list_attack)<br>    parse_sequence(ball, list_ball)</pre>\n<p>Finally, for the second moment of truth in this analysis, heatmap for the positions of the\u00a0players</p>\n<pre>def createGrid(values, title_str):<br>    v = np.asarray(values)<br>    plt.pcolor(v)<br>    plt.title(title_str)<br>    plt.legend()<br>    plt.show()<br>createGrid(values, \"Heatmap for player's position on pitch\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5zOiV8B_jeru-XUUQCAVXA.png\"><figcaption>Look at this thing of beauty, heat map for positions of players for all the sequences of\u00a0play</figcaption></figure><p>Now moving on, I would like to see the passes made in the sequences, and the heatmap for position of the ball throughout the\u00a0game.</p>\n<pre>createGrid(ball, \"Heatmap for ball position on pitch\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SQN0_VVObAgOKUjpYphzjw.png\"><figcaption>Heat map for position of the ball throughout the entire\u00a0game</figcaption></figure><h3>KMeans clustering analysis</h3>\n<p>Clearly having a set of points that I could cluster, I released that KMeans is the easiest way to cluster this data to show some interesting insights into the\u00a0data</p>\n<p><strong>Reasons why I chose\u00a0KMeans</strong></p>\n<ul>\n<li>KMeans clustering is used to classify unstructured data\u200a\u2014\u200ahaving unstructured data with two features, the X and Y coordinate of each player\u2019s position suited well for the\u00a0analysis</li>\n<li>KMeans works by finding the centroids of the feature mappings of the data\u200a\u2014\u200aI am using it to find the centroid of the X,Y coordinates of the player\u2019s positions, this provides an insight into the relative structure of the team for the attacking and defensive team</li>\n<li>Using KMeans on positions of both defensive and attacking players would shed light on the natural positions of the pitch in which the game is contested in</li>\n</ul>\n<p>Lets get down to the code, this is for obtaining all the data to feed into our KMeans classifier</p>\n<pre>from sklearn.cluster import KMeans</pre>\n<pre>list_def = []<br>list_att = []<br>list_bll = []<br>for i in range(len(sequenced_data)):<br>    list_defense, list_attack, list_ball = parse_data(sequenced_data,i)<br>    list_def = list_def + list_defense<br>    list_att = list_att + list_attack<br>    list_bll = list_bll + list_ball</pre>\n<p>Now over to the KMeans classifier and visualizing code. The notebook is also available to over\u00a0<a href=\"https://github.com/raghav96/datascience/blob/master/Football%20Analysis.ipynb\">here</a>.</p>\n<pre>def plotKMeans(train, data, title_str, k=11):<br>    # Number of clusters<br>    kmeans = KMeans(n_clusters=k)<br>    # Fitting the input data<br>    kmeans = kmeans.fit(train)<br>    # Getting the cluster labels<br>    labels = kmeans.predict(data)<br>    # Centroid values<br>    centroids = kmeans.cluster_centers_<br>    def xy(lst):<br>        x = [l[0] for l in lst]<br>        y = [l[1] for l in lst]<br>        return x, y<br>    x, y = xy(data)<br>    fig, ax = plt.subplots()<br>    if (title_str[0] == 'D'):<br>        ax.scatter(x, y, marker='o', color='#ffff00', alpha=0.03)<br>    else:<br>        ax.scatter(x, y, marker='o', color='#00ffff', alpha=0.03)</pre>\n<pre>ax.scatter(centroids[:,0], y=centroids[:,1], marker='*', c='#050505', s=100)<br>    plt.title(title_str)<br>    plt.legend()<br>    plt.show()<br>    return centroids</pre>\n<h4>KMeans centroids for defense and attacking team</h4>\n<p>And now plotting the KMeans centroids and positions of the defensive team</p>\n<pre>def_average_pos = plotKMeans(list_def, list_def, \"Defensive team average positions\")<br>att_average_pos = plotKMeans(list_att, list_att, \"Attacking team average positions\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*klx_PpFtQD2uXeHpQq-Utw.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*rBV4rNY9NFXK9T_fuf3jZw.png\"><figcaption>These do indeed look like formations, but we\u2019ll come to that in a\u00a0second.</figcaption></figure><h4>Relationship to\u00a0ball</h4>\n<p>So this time, I tried to see how the positions of the ball might affect the KMeans clusters of both the attacking and defensive team, but it doesn\u2019t seem to have much\u00a0effect</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*2tr-T29N_SQpysm5X6glyQ.png\"></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*8PhHvAypoTvypHH8syxTjw.png\"><figcaption>Not much of a difference eh?</figcaption></figure><h4>Areas of pitch with most\u00a0activity</h4>\n<p>Now let us find out which parts of the pitch were the most occupied, an insight to see the nature of the game at large. Here I used the number of clusters to be 10 for the entire dataset, hoping to see 10 centroids on the pitch players of both teams are occupying. Here is the code for\u00a0this</p>\n<pre>plotKMeans(list_def+list_att, list_def+list_att, \"Areas of the pitch with most activity\", 10)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*xKS_0tVhs6btU7VuM4kvzQ.png\"><figcaption>This resulted in this heat map showing 10 positions (KMeans centroids) on the pitch that players of both teams\u00a0occupy</figcaption></figure><h4>Formations</h4>\n<p>Now finally coming to the obvious here, we can see on using KMeans clustering with 11 clusters, we could find the positioning of each player of the defending and attacking team, creating a formation. Now I took the centroids from the defending and attacking teams\u2019 positions and plotted them, they came up looking just like a formation. Putting this on the football pitch from the earlier part of this analysis put it really into\u00a0context.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*n8aaa8P3yk_jtRtPEEPW3A.png\"><figcaption>My first try to plot the centroids of the attacking and defending team</figcaption></figure><p>Nice! To my trained eye, I can see a defense lining up as 3\u20133\u20133\u20131 formation vs attack lining up as 4\u20132\u20133\u20131 formation with the full-backs pushed up higher, let me display that on a\u00a0pitch</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/380/1*7y9vI24f9YHDr-kOkHfMwg.png\"><figcaption>The attack is red and defense is\u00a0blue.</figcaption></figure><h3>The End</h3>\n<p>There we go, from the positions of players on the pitch, we have analyzed sequences of play for different metrics, how the players moved with respect to the ball, with respect to the opposing team, tried to build a feature, analyzed positions occupied on the pitch, tried to classify the data using KMeans clustering and ended up with this visualization of formations using KMeans centroids</p>\n<p>It was a challenging dataset to work with, but we came up with some interesting results at the end. I look forward to your comments on this <a href=\"https://github.com/raghav96/datascience/blob/master/Football%20Analysis.ipynb\">notebook</a> and suggest more\u00a0tips.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=f903d2c9e2ff\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://towardsdatascience.com/the-football-derby-a-data-science-story-f903d2c9e2ff\">The Football Derby \u2014 A Data Science Story</a> was originally published in <a href=\"https://towardsdatascience.com/\">Towards Data Science</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["data-science","towards-data-science","k-means","football","data-visualization"]},{"title":"What the $@#&amp;*?!?","pubDate":"2018-08-12 03:04:54","link":"https://raghavism.medium.com/what-the-3d5a12b06f50?source=rss-226ffefb5260------2","guid":"https://medium.com/p/3d5a12b06f50","author":"Raghav Ravisankar","thumbnail":"","description":"\n<h3>What the $@#&amp;*?!?\u200a\u2014\u200aIdentifying Toxic Comments using Naives Bayes Linear Regression and Keras LSTM\u00a0models</h3>\n<p>In this notebook we would be exploring the world of text based analysis in the world of machine learning. In this analysis I looked to learn as much as I can in the methods used to do text analysis and tried to build a predictor based on comment data from the Google Jigsaw Toxic Comment Classification Challenge from Kaggle <a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\">here</a>. You can find the dataset here and the Jupyter notebook on my Github right\u00a0<a href=\"https://github.com/raghav96/datascience/blob/master/Toxic%20Comment%20Dataset%20Challenge.ipynb\">here</a>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*b8UTKmy3DJ69oScT-L45IQ.png\"><figcaption>A word cloud that is part of this\u00a0analysis</figcaption></figure><p>This Jupyter notebook is split into two parts, a set of LogisticRegression models which predict based on a text model that is built from CountVectorizer and TfidfVectorizer packages in python and a LSTM model to model the data as a Linear Time Series\u00a0model.</p>\n<p><strong>Importing the packages and reading the dataset from\u00a0file</strong></p>\n<pre>import pandas as pd, numpy as np<br>from matplotlib import pyplot as plt<br>from sklearn.linear_model import LogisticRegression<br>from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer</pre>\n<p>Let us now import the data into a Pandas dataframe and look at the\u00a0dataset.</p>\n<pre>train = pd.read_csv('toxic/train.csv')<br>test = pd.read_csv('toxic/test.csv')<br>train.head()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Wn44NaniPOzZeJ7Bq8wZSg.png\"><figcaption>Now let us take a look at this dataset and find out how the columns look\u00a0like</figcaption></figure><p>When we look at at text, or as you are reading this document right now, this makes sense because we have complex models in our brain that model the meaning of these words. But how do we make a computer understand set of words and meanings? The answer is <strong>Word Embedding.</strong></p>\n<blockquote>Sure, a computer can match two strings and tell you whether they are same or not. But how do we make computers tell you about football or Ronaldo when you search for Messi? How do you make a computer understand that \u201cApple\u201d in \u201cApple is a tasty fruit\u201d is a fruit that can be eaten and not a\u00a0company?</blockquote>\n<blockquote>The answer to the above questions lie in creating a representation for words that capture their <em>meanings</em>, <em>semantic relationships</em> and the different types of contexts they are used\u00a0in.</blockquote>\n<p>Now let us look at the length of each comment and how they look on a histogram.</p>\n<pre>lengths = train.comment_text.str.len()<br>lengths.mean(), lengths.std(), lengths.max()<br>lengths.hist()<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/948/1*039zAtMalbW2xfsQNvTm6Q.png\"><figcaption>Let us look at the relationship between the length of a comment and the number of comments which are of that size, obviously, we can understand from this graph that most comments are\u00a0small.</figcaption></figure><p><strong>WordCloud</strong></p>\n<p>Now one fun thing that I have always wanted to do when building any analysis in text is to build a wordcloud. So please find the code for that\u00a0below.</p>\n<pre>from wordcloud import WordCloud, STOPWORDS<br><br>comment_words = ' '<br>stopwords = set(STOPWORDS)</pre>\n<pre># iterate through the csv file<br>for val in comments_train[0:200]:<br><br>    # typecaste each val to string<br>    val = str(val)<br>    tokens = tokenize(val.strip(string.punctuation))<br><br>    # Converts each token into lowercase<br>    for i in range(len(tokens)):<br>        tokens[i] = tokens[i].lower()<br><br>    for words in tokens:<br>        comment_words = comment_words + words + ' '<br><br><br>wordcloud = WordCloud(width = 800, height = 800,<br>                background_color ='white',<br>                stopwords = stopwords,<br>                min_font_size = 10).generate(comment_words)<br><br># plot the WordCloud image                       <br>plt.figure(figsize = (8, 8), facecolor = None)<br>plt.imshow(wordcloud)<br>plt.axis(\"off\")<br>plt.tight_layout(pad = 0)<br><br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*b8UTKmy3DJ69oScT-L45IQ.png\"><figcaption>Word cloud from the first 200 comments of the\u00a0dataset</figcaption></figure><p>Here we can clearly see the most common words amongst the first 200 comments in the dataset. Here is the wordcloud that I generated from this. We can clearly see the premise of the dataset, it is clear that we have these comments from articles from Wikipedia.</p>\n<p>Now I am looking forward to building a predictive model from this dataset and jump right in\u00a0now.</p>\n<p><strong>CountVectorizer and TfidfVectorizer</strong></p>\n<p>Before building this part of the analysis, I sought out a detailed understanding of the methods used to build statistical models from the comments that are provided to us. Here we use <em>CountVectorizer</em> and <em>TfidfVectorizer</em>, so what are\u00a0they?</p>\n<p>So now let us take a closer look at these\u00a0two.</p>\n<p>Consider the corpus of all the comments in the dataset to be <em>C</em> and the list of comments here to be <em>(d(1), d(2),\u00a0\u2026 d(D))</em>. Let us consider that we have <em>N</em> unique words in corpus of the dataset. Let us consider the size of the count vector matrix <em>M</em> which would hence have the dimensions <em>D x N. </em>Each row in the matrix M contains the frequency of tokens in comment\u00a0<em>D(i)</em>.</p>\n<p>Let us understand this using a simple\u00a0example.</p>\n<p>D1: The quick fox jumped over the lazy\u00a0dog.</p>\n<p>D2: The quick dog jumped over the lazy\u00a0cat.</p>\n<p>The dictionary created may be a list of unique tokens(words) in the corpus\u00a0=</p>\n<p>[\u2018The\u2019, \u2018quick\u2019, \u2018fox\u2019, \u2018lazy\u2019, \u2018dog\u2019, \u2018cat\u2019, \u2018jumped\u2019, \u2018over\u2019,\u00a0\u2018the\u2019]</p>\n<p>Here the matrix has two comments and nine unique words so, D=2,\u00a0N=9</p>\n<p>So each comment here in this corpus would be encoded as a vector according the frequency of the word occurring the corpus occurring in the comment. They have one for all the values except the frequency of the term \u2018cat\u2019 and \u2018fox\u2019. D1 has the frequency of cat to be 0 and frequency of the fox is 1. D2 has the frequency of cat to be 1and frequency of the fox is\u00a00.</p>\n<p>Now that we understand CountVectorizer, let us look at TfidfVectorizer. So what does TF-IDF stand for. Here\u2019s a simple\u00a0formula:</p>\n<p>IDF = <em>log(N/t(n))</em>, where, <em>N</em> is the number of documents and <em>t(n)</em> is the number of documents a term <em>t</em> has appeared\u00a0in.</p>\n<p>Now TF-IDF simply means <em>term frequency(TF), inverse-document frequency(IDF). </em>Term frequency is the number of times a term appears in a comment. Inverse document frequency is the log of the ratio between the number of times the word appears in the corpus to the number of times it appears on a comment. Okay now let\u2019s move onto the actual code to build the TF-IDF\u00a0model.</p>\n<pre>n = train.shape[0]<br>vec = TfidfVectorizer(ngram_range=(1,3), tokenizer=tokenize,<br> min_df=3, max_df=0.9, strip_accents=\u2019unicode\u2019, use_idf=1,<br> smooth_idf=1, sublinear_tf=1 )<br>X_train = vec.fit_transform(train[\u201ccomment_text\u201d])<br>X_test = vec.transform(test[\u201ccomment_text\u201d])</pre>\n<p>Okay, here the package TfidfVectorizer takes in multiple parameters, let us take a closer look at\u00a0that.</p>\n<p>Here we would be using the Tfidf vectorizer to build our bag-of-words model.</p>\n<ul>\n<li>\n<strong>ngram_range\u00a0:</strong> creates ngrams(weighted sequences of text of length\u00a0(1,3).</li>\n<li>\n<strong>Min_df and max_df\u00a0:</strong> For the values to make sense heres the formula for inverse document frequency</li>\n<li>\n<strong>Tokenizer\u00a0: </strong>tokenize is a function that makes the comments into a vector of word embedddings</li>\n</ul>\n<p>Now, let us take a look at the formula of inverse document frequency.</p>\n<p><em>IDF(t) = log_e(Total number of documents / Number of documents with term t in\u00a0it).</em></p>\n<ul><li>The log_e function should give a clue unto the value of the parameters <em>min_df</em> and <em>max_df</em> in the function. These are cutoff values because they restrict the idf values accepted by the vectorizer.</li></ul>\n<p>Cut off idf values are used in order to remove words that\u00a0are:</p>\n<ul>\n<li>insignificant in occurence</li>\n<li>stopwords (words like \u2018at\u2019, \u2018a\u2019, \u2018in\u2019) that repeat too many items to be statistically significant</li>\n</ul>\n<p>More information on the TF-IDF vectorizer is available at <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a></p>\n<p><strong>Naives Bayes\u00a0Model</strong></p>\n<p>Now let us define a Naive Bayes function to find out the probability of an event based on the previous events that occured. This is the code for the posterior probability of an event <em>y_i</em> given the list of events\u00a0<em>y</em>.</p>\n<pre>def prob(y_i, y, x):<br>    p = x[y==y_i].sum(0)<br>    return (p+1) / ((y==y_i).sum()+1)</pre>\n<p>Now let us define a function to build a LogisticRegression model based on the data. The parameter C in LogisticRegression is the regularization constant in the LogisticRegressor and in this case, the lower the constant, the higher the regularization.</p>\n<pre>def get_model(y):<br> y = y.values<br> r = np.log(prob(1,y, X_train) / prob(0,y, X_train))<br> m = LogisticRegression(C=4)<br> x_nb = X_train.multiply(r)<br> return m.fit(x_nb, y), r</pre>\n<p>Now let us start prediction and store the probability predictions in a newly initialized array <em>preds. </em>Here, each column in the dataset, in this case the columns we are predicting, have probabilities predicted for the column and the predictions are stored in the array. Now <em>preds </em>contains the predictions for each\u00a0column.</p>\n<pre>preds = np.zeros((len(test), len(cols)))</pre>\n<pre>for i, col in enumerate(cols):<br>    m,r = get_model(train[col])<br>    preds[:,i] = m.predict_proba(X_test.multiply(r))[:,1]</pre>\n<p>Here is how we build our submission file for this methods of prediction</p>\n<pre>subm_id = pd.DataFrame({'id': subm[\"id\"]})<br>submission = pd.concat([subm_id, pd.DataFrame(preds, columns = cols)], axis=1)<br>submission.to_csv('toxic/submission.csv', index=False)</pre>\n<p><strong>LSTM Model with\u00a0Keras</strong></p>\n<p>So now that we have build a Naive Bayes model, let us move forward to building a LSTM model with Keras in order to learn how to build a LSTM neural network to predict the category of comment. Here are the necessary imports to start off this part of the analysis.</p>\n<pre>from keras.preprocessing.text import Tokenizer<br>from keras.preprocessing.sequence import pad_sequences<br>from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D<br>from keras.layers import Bidirectional, GlobalMaxPool1D<br>from keras.models import Model<br>from keras import initializers, regularizers, constraints, optimizers, layers<br>from keras import Sequential</pre>\n<p>So first step, let us look at tokenizing the comments available on the dataset to build a feature for the LSTM neural network from the dataset. The two things that we need in a feature for a neural network\u00a0are:</p>\n<ul>\n<li>Fixed length of\u00a0feature</li>\n<li>The values in the dataset are properly represented in the\u00a0feature.</li>\n</ul>\n<p>Here is the code to build the feature using the Tokenizer package.</p>\n<pre>max_features = 20000<br>tokenizer = Tokenizer(num_words=max_features)<br>tokenizer.fit_on_texts(list(comments_train))<br>list_tok_train = tokenizer.texts_to_sequences(comments_train)<br>list_tok_test = tokenizer.texts_to_sequences(comments_test)</pre>\n<p>Now let us look at the length of these sequences and how they are structured</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/930/1*fSixLnmfKcOKmBSjux8Nsg.png\"><figcaption>A lot of small comments and lesser long comments, our cutoff value be large enough to capture a good length of a comment and small enough to capture the variations in the comments. Let us be paranoid and choose 200\u00a0here.</figcaption></figure><p>Here we have word embedded sequences of comments of the dataset. Now let us pad these sequences so they are fixed in\u00a0length.</p>\n<pre>maxlen = 200<br>X_t = pad_sequences(list_tok_train, maxlen=maxlen)<br>X_te = pad_sequences(list_tok_test, maxlen=maxlen)</pre>\n<p>Now, time to build the LSTM model. Here is the code for building the LSTM model. In this analysis I would be using the LSTM model in Keras, we have 5\u00a0layers,</p>\n<ul>\n<li>an input layer to input the data into the neural\u00a0network,</li>\n<li>an embedding layer to build a 3d vector model for the representation of a relationship between words, explained in detail\u00a0<a href=\"https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12\">here</a>\n</li>\n<li>a Long-Short Term Memory layer to represent the series like nature of sentences in\u00a0data,</li>\n<li>a max-pooling layer to reduce the dimensionality of the features\u00a0learned,</li>\n<li>a dropout layer to remove some of the dimensionality of the neural\u00a0network</li>\n<li>an activation layer to output the prediction of the neural\u00a0network</li>\n</ul>\n<pre>embed_size = 128<br>hidden_layer = 60<br>dropout = 0.1<br>output_layer = 6</pre>\n<pre>def buildModel(maxlen = 200, max_features = 20000, embed_size = 128, hidden_layer = 60, dropout = 0.1, output_layer = 6):<br>    model = Sequential()<br>    inp = Input(shape=(maxlen,))<br>    x = Embedding(max_features, embed_size)(inp)<br>    x = LSTM(hidden_layer, return_sequences=True,name='lstm_layer')(x)<br>    x = GlobalMaxPool1D()(x)<br>    x = Dropout(dropout)(x)<br>    x = Dense(output_layer, activation=\"sigmoid\")(x)<br>    model = Model(inputs=inp, outputs=x)<br>    model.compile(loss='binary_crossentropy',<br>                  optimizer='adam',<br>                  metrics=['accuracy'])<br>    model.summary()<br>    return model<br><br>model = buildModel(maxlen, max_features, embed_size, hidden_layer, dropout, output_layer)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4jq41W9EAW8SmAZTU_ILtw.png\"><figcaption>Result from model.summary(). We have an input layer, an embedding layer, a lstm layer, a global max pooling layer, a dropout layer and a dense\u00a0layer.</figcaption></figure><p>Now let us predict the values, here is the code for prediction.</p>\n<pre>batch_size = 32<br>epochs = 2<br>model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)</pre>\n<pre>[Output]</pre>\n<pre># Train on 143613 samples, validate on 15958 samples<br># Epoch 1/2<br># 143613/143613 [==============================] - 1275s 9ms/step - # loss: 0.0729 - acc: 0.9767 - val_loss: 0.0508 - val_acc: 0.9818<br># Epoch 2/2<br># 143613/143613 [==============================] - 1199s 8ms/step - # loss: 0.0459 - acc: 0.9832 - val_loss: 0.0486 - val_acc: 0.9822</pre>\n<p>In order to find out the accuracy of the data on the validation, we would be taking a small amount of data from the test set to find out the score of the model on the validation set.</p>\n<pre>validation_size = 1500<br>X_val = X_te[-validation_size:]<br>y_val = y[-validation_size:]<br>score,acc = model.evaluate(X_val, y_val, verbose = 2, batch_size = batch_size)<br>print(\"score: %.2f\" % (score))<br>print(\"acc: %.2f\" % (acc))</pre>\n<p>In this analysis I got an accuracy of\u00a089%.</p>\n<p>I guess this sums up my prediction of toxic comments from the Jigsaw Toxic Comment Challenge. Some of the methods and practices that I have used in this notebook are inspired by \u201cJeremy Howard\u201d in his <a href=\"https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline\">Kaggle notebook</a> and \u201cBongo\u201d in his <a href=\"https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras.\">notebook</a>.</p>\n<p>If you like my articles, please follow me on Medium. If you would like to take a look at the Jupyter notebook for this analysis please look <a href=\"https://github.com/raghav96/datascience/blob/master/Toxic%20Comment%20Dataset%20Challenge.ipynb\">here</a>. I\u2019m interested in collaborating to work on more data science projects! Please contact me if you would like to work any interesting datasets with me. I look forward to your opinion and comments.</p>\n<p><strong>References</strong></p>\n<ol>\n<li><a href=\"https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\">https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/</a></li>\n<li><a href=\"https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline\">https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline</a></li>\n<li><a href=\"https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras\">https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras</a></li>\n</ol>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3d5a12b06f50\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>What the $@#&amp;*?!?\u200a\u2014\u200aIdentifying Toxic Comments using Naives Bayes Linear Regression and Keras LSTM\u00a0models</h3>\n<p>In this notebook we would be exploring the world of text based analysis in the world of machine learning. In this analysis I looked to learn as much as I can in the methods used to do text analysis and tried to build a predictor based on comment data from the Google Jigsaw Toxic Comment Classification Challenge from Kaggle <a href=\"https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\">here</a>. You can find the dataset here and the Jupyter notebook on my Github right\u00a0<a href=\"https://github.com/raghav96/datascience/blob/master/Toxic%20Comment%20Dataset%20Challenge.ipynb\">here</a>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*b8UTKmy3DJ69oScT-L45IQ.png\"><figcaption>A word cloud that is part of this\u00a0analysis</figcaption></figure><p>This Jupyter notebook is split into two parts, a set of LogisticRegression models which predict based on a text model that is built from CountVectorizer and TfidfVectorizer packages in python and a LSTM model to model the data as a Linear Time Series\u00a0model.</p>\n<p><strong>Importing the packages and reading the dataset from\u00a0file</strong></p>\n<pre>import pandas as pd, numpy as np<br>from matplotlib import pyplot as plt<br>from sklearn.linear_model import LogisticRegression<br>from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer</pre>\n<p>Let us now import the data into a Pandas dataframe and look at the\u00a0dataset.</p>\n<pre>train = pd.read_csv('toxic/train.csv')<br>test = pd.read_csv('toxic/test.csv')<br>train.head()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Wn44NaniPOzZeJ7Bq8wZSg.png\"><figcaption>Now let us take a look at this dataset and find out how the columns look\u00a0like</figcaption></figure><p>When we look at at text, or as you are reading this document right now, this makes sense because we have complex models in our brain that model the meaning of these words. But how do we make a computer understand set of words and meanings? The answer is <strong>Word Embedding.</strong></p>\n<blockquote>Sure, a computer can match two strings and tell you whether they are same or not. But how do we make computers tell you about football or Ronaldo when you search for Messi? How do you make a computer understand that \u201cApple\u201d in \u201cApple is a tasty fruit\u201d is a fruit that can be eaten and not a\u00a0company?</blockquote>\n<blockquote>The answer to the above questions lie in creating a representation for words that capture their <em>meanings</em>, <em>semantic relationships</em> and the different types of contexts they are used\u00a0in.</blockquote>\n<p>Now let us look at the length of each comment and how they look on a histogram.</p>\n<pre>lengths = train.comment_text.str.len()<br>lengths.mean(), lengths.std(), lengths.max()<br>lengths.hist()<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/948/1*039zAtMalbW2xfsQNvTm6Q.png\"><figcaption>Let us look at the relationship between the length of a comment and the number of comments which are of that size, obviously, we can understand from this graph that most comments are\u00a0small.</figcaption></figure><p><strong>WordCloud</strong></p>\n<p>Now one fun thing that I have always wanted to do when building any analysis in text is to build a wordcloud. So please find the code for that\u00a0below.</p>\n<pre>from wordcloud import WordCloud, STOPWORDS<br><br>comment_words = ' '<br>stopwords = set(STOPWORDS)</pre>\n<pre># iterate through the csv file<br>for val in comments_train[0:200]:<br><br>    # typecaste each val to string<br>    val = str(val)<br>    tokens = tokenize(val.strip(string.punctuation))<br><br>    # Converts each token into lowercase<br>    for i in range(len(tokens)):<br>        tokens[i] = tokens[i].lower()<br><br>    for words in tokens:<br>        comment_words = comment_words + words + ' '<br><br><br>wordcloud = WordCloud(width = 800, height = 800,<br>                background_color ='white',<br>                stopwords = stopwords,<br>                min_font_size = 10).generate(comment_words)<br><br># plot the WordCloud image                       <br>plt.figure(figsize = (8, 8), facecolor = None)<br>plt.imshow(wordcloud)<br>plt.axis(\"off\")<br>plt.tight_layout(pad = 0)<br><br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*b8UTKmy3DJ69oScT-L45IQ.png\"><figcaption>Word cloud from the first 200 comments of the\u00a0dataset</figcaption></figure><p>Here we can clearly see the most common words amongst the first 200 comments in the dataset. Here is the wordcloud that I generated from this. We can clearly see the premise of the dataset, it is clear that we have these comments from articles from Wikipedia.</p>\n<p>Now I am looking forward to building a predictive model from this dataset and jump right in\u00a0now.</p>\n<p><strong>CountVectorizer and TfidfVectorizer</strong></p>\n<p>Before building this part of the analysis, I sought out a detailed understanding of the methods used to build statistical models from the comments that are provided to us. Here we use <em>CountVectorizer</em> and <em>TfidfVectorizer</em>, so what are\u00a0they?</p>\n<p>So now let us take a closer look at these\u00a0two.</p>\n<p>Consider the corpus of all the comments in the dataset to be <em>C</em> and the list of comments here to be <em>(d(1), d(2),\u00a0\u2026 d(D))</em>. Let us consider that we have <em>N</em> unique words in corpus of the dataset. Let us consider the size of the count vector matrix <em>M</em> which would hence have the dimensions <em>D x N. </em>Each row in the matrix M contains the frequency of tokens in comment\u00a0<em>D(i)</em>.</p>\n<p>Let us understand this using a simple\u00a0example.</p>\n<p>D1: The quick fox jumped over the lazy\u00a0dog.</p>\n<p>D2: The quick dog jumped over the lazy\u00a0cat.</p>\n<p>The dictionary created may be a list of unique tokens(words) in the corpus\u00a0=</p>\n<p>[\u2018The\u2019, \u2018quick\u2019, \u2018fox\u2019, \u2018lazy\u2019, \u2018dog\u2019, \u2018cat\u2019, \u2018jumped\u2019, \u2018over\u2019,\u00a0\u2018the\u2019]</p>\n<p>Here the matrix has two comments and nine unique words so, D=2,\u00a0N=9</p>\n<p>So each comment here in this corpus would be encoded as a vector according the frequency of the word occurring the corpus occurring in the comment. They have one for all the values except the frequency of the term \u2018cat\u2019 and \u2018fox\u2019. D1 has the frequency of cat to be 0 and frequency of the fox is 1. D2 has the frequency of cat to be 1and frequency of the fox is\u00a00.</p>\n<p>Now that we understand CountVectorizer, let us look at TfidfVectorizer. So what does TF-IDF stand for. Here\u2019s a simple\u00a0formula:</p>\n<p>IDF = <em>log(N/t(n))</em>, where, <em>N</em> is the number of documents and <em>t(n)</em> is the number of documents a term <em>t</em> has appeared\u00a0in.</p>\n<p>Now TF-IDF simply means <em>term frequency(TF), inverse-document frequency(IDF). </em>Term frequency is the number of times a term appears in a comment. Inverse document frequency is the log of the ratio between the number of times the word appears in the corpus to the number of times it appears on a comment. Okay now let\u2019s move onto the actual code to build the TF-IDF\u00a0model.</p>\n<pre>n = train.shape[0]<br>vec = TfidfVectorizer(ngram_range=(1,3), tokenizer=tokenize,<br> min_df=3, max_df=0.9, strip_accents=\u2019unicode\u2019, use_idf=1,<br> smooth_idf=1, sublinear_tf=1 )<br>X_train = vec.fit_transform(train[\u201ccomment_text\u201d])<br>X_test = vec.transform(test[\u201ccomment_text\u201d])</pre>\n<p>Okay, here the package TfidfVectorizer takes in multiple parameters, let us take a closer look at\u00a0that.</p>\n<p>Here we would be using the Tfidf vectorizer to build our bag-of-words model.</p>\n<ul>\n<li>\n<strong>ngram_range\u00a0:</strong> creates ngrams(weighted sequences of text of length\u00a0(1,3).</li>\n<li>\n<strong>Min_df and max_df\u00a0:</strong> For the values to make sense heres the formula for inverse document frequency</li>\n<li>\n<strong>Tokenizer\u00a0: </strong>tokenize is a function that makes the comments into a vector of word embedddings</li>\n</ul>\n<p>Now, let us take a look at the formula of inverse document frequency.</p>\n<p><em>IDF(t) = log_e(Total number of documents / Number of documents with term t in\u00a0it).</em></p>\n<ul><li>The log_e function should give a clue unto the value of the parameters <em>min_df</em> and <em>max_df</em> in the function. These are cutoff values because they restrict the idf values accepted by the vectorizer.</li></ul>\n<p>Cut off idf values are used in order to remove words that\u00a0are:</p>\n<ul>\n<li>insignificant in occurence</li>\n<li>stopwords (words like \u2018at\u2019, \u2018a\u2019, \u2018in\u2019) that repeat too many items to be statistically significant</li>\n</ul>\n<p>More information on the TF-IDF vectorizer is available at <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html</a></p>\n<p><strong>Naives Bayes\u00a0Model</strong></p>\n<p>Now let us define a Naive Bayes function to find out the probability of an event based on the previous events that occured. This is the code for the posterior probability of an event <em>y_i</em> given the list of events\u00a0<em>y</em>.</p>\n<pre>def prob(y_i, y, x):<br>    p = x[y==y_i].sum(0)<br>    return (p+1) / ((y==y_i).sum()+1)</pre>\n<p>Now let us define a function to build a LogisticRegression model based on the data. The parameter C in LogisticRegression is the regularization constant in the LogisticRegressor and in this case, the lower the constant, the higher the regularization.</p>\n<pre>def get_model(y):<br> y = y.values<br> r = np.log(prob(1,y, X_train) / prob(0,y, X_train))<br> m = LogisticRegression(C=4)<br> x_nb = X_train.multiply(r)<br> return m.fit(x_nb, y), r</pre>\n<p>Now let us start prediction and store the probability predictions in a newly initialized array <em>preds. </em>Here, each column in the dataset, in this case the columns we are predicting, have probabilities predicted for the column and the predictions are stored in the array. Now <em>preds </em>contains the predictions for each\u00a0column.</p>\n<pre>preds = np.zeros((len(test), len(cols)))</pre>\n<pre>for i, col in enumerate(cols):<br>    m,r = get_model(train[col])<br>    preds[:,i] = m.predict_proba(X_test.multiply(r))[:,1]</pre>\n<p>Here is how we build our submission file for this methods of prediction</p>\n<pre>subm_id = pd.DataFrame({'id': subm[\"id\"]})<br>submission = pd.concat([subm_id, pd.DataFrame(preds, columns = cols)], axis=1)<br>submission.to_csv('toxic/submission.csv', index=False)</pre>\n<p><strong>LSTM Model with\u00a0Keras</strong></p>\n<p>So now that we have build a Naive Bayes model, let us move forward to building a LSTM model with Keras in order to learn how to build a LSTM neural network to predict the category of comment. Here are the necessary imports to start off this part of the analysis.</p>\n<pre>from keras.preprocessing.text import Tokenizer<br>from keras.preprocessing.sequence import pad_sequences<br>from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D<br>from keras.layers import Bidirectional, GlobalMaxPool1D<br>from keras.models import Model<br>from keras import initializers, regularizers, constraints, optimizers, layers<br>from keras import Sequential</pre>\n<p>So first step, let us look at tokenizing the comments available on the dataset to build a feature for the LSTM neural network from the dataset. The two things that we need in a feature for a neural network\u00a0are:</p>\n<ul>\n<li>Fixed length of\u00a0feature</li>\n<li>The values in the dataset are properly represented in the\u00a0feature.</li>\n</ul>\n<p>Here is the code to build the feature using the Tokenizer package.</p>\n<pre>max_features = 20000<br>tokenizer = Tokenizer(num_words=max_features)<br>tokenizer.fit_on_texts(list(comments_train))<br>list_tok_train = tokenizer.texts_to_sequences(comments_train)<br>list_tok_test = tokenizer.texts_to_sequences(comments_test)</pre>\n<p>Now let us look at the length of these sequences and how they are structured</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/930/1*fSixLnmfKcOKmBSjux8Nsg.png\"><figcaption>A lot of small comments and lesser long comments, our cutoff value be large enough to capture a good length of a comment and small enough to capture the variations in the comments. Let us be paranoid and choose 200\u00a0here.</figcaption></figure><p>Here we have word embedded sequences of comments of the dataset. Now let us pad these sequences so they are fixed in\u00a0length.</p>\n<pre>maxlen = 200<br>X_t = pad_sequences(list_tok_train, maxlen=maxlen)<br>X_te = pad_sequences(list_tok_test, maxlen=maxlen)</pre>\n<p>Now, time to build the LSTM model. Here is the code for building the LSTM model. In this analysis I would be using the LSTM model in Keras, we have 5\u00a0layers,</p>\n<ul>\n<li>an input layer to input the data into the neural\u00a0network,</li>\n<li>an embedding layer to build a 3d vector model for the representation of a relationship between words, explained in detail\u00a0<a href=\"https://towardsdatascience.com/deep-learning-4-embedding-layers-f9a02d55ac12\">here</a>\n</li>\n<li>a Long-Short Term Memory layer to represent the series like nature of sentences in\u00a0data,</li>\n<li>a max-pooling layer to reduce the dimensionality of the features\u00a0learned,</li>\n<li>a dropout layer to remove some of the dimensionality of the neural\u00a0network</li>\n<li>an activation layer to output the prediction of the neural\u00a0network</li>\n</ul>\n<pre>embed_size = 128<br>hidden_layer = 60<br>dropout = 0.1<br>output_layer = 6</pre>\n<pre>def buildModel(maxlen = 200, max_features = 20000, embed_size = 128, hidden_layer = 60, dropout = 0.1, output_layer = 6):<br>    model = Sequential()<br>    inp = Input(shape=(maxlen,))<br>    x = Embedding(max_features, embed_size)(inp)<br>    x = LSTM(hidden_layer, return_sequences=True,name='lstm_layer')(x)<br>    x = GlobalMaxPool1D()(x)<br>    x = Dropout(dropout)(x)<br>    x = Dense(output_layer, activation=\"sigmoid\")(x)<br>    model = Model(inputs=inp, outputs=x)<br>    model.compile(loss='binary_crossentropy',<br>                  optimizer='adam',<br>                  metrics=['accuracy'])<br>    model.summary()<br>    return model<br><br>model = buildModel(maxlen, max_features, embed_size, hidden_layer, dropout, output_layer)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4jq41W9EAW8SmAZTU_ILtw.png\"><figcaption>Result from model.summary(). We have an input layer, an embedding layer, a lstm layer, a global max pooling layer, a dropout layer and a dense\u00a0layer.</figcaption></figure><p>Now let us predict the values, here is the code for prediction.</p>\n<pre>batch_size = 32<br>epochs = 2<br>model.fit(X_t,y, batch_size=batch_size, epochs=epochs, validation_split=0.1)</pre>\n<pre>[Output]</pre>\n<pre># Train on 143613 samples, validate on 15958 samples<br># Epoch 1/2<br># 143613/143613 [==============================] - 1275s 9ms/step - # loss: 0.0729 - acc: 0.9767 - val_loss: 0.0508 - val_acc: 0.9818<br># Epoch 2/2<br># 143613/143613 [==============================] - 1199s 8ms/step - # loss: 0.0459 - acc: 0.9832 - val_loss: 0.0486 - val_acc: 0.9822</pre>\n<p>In order to find out the accuracy of the data on the validation, we would be taking a small amount of data from the test set to find out the score of the model on the validation set.</p>\n<pre>validation_size = 1500<br>X_val = X_te[-validation_size:]<br>y_val = y[-validation_size:]<br>score,acc = model.evaluate(X_val, y_val, verbose = 2, batch_size = batch_size)<br>print(\"score: %.2f\" % (score))<br>print(\"acc: %.2f\" % (acc))</pre>\n<p>In this analysis I got an accuracy of\u00a089%.</p>\n<p>I guess this sums up my prediction of toxic comments from the Jigsaw Toxic Comment Challenge. Some of the methods and practices that I have used in this notebook are inspired by \u201cJeremy Howard\u201d in his <a href=\"https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline\">Kaggle notebook</a> and \u201cBongo\u201d in his <a href=\"https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras.\">notebook</a>.</p>\n<p>If you like my articles, please follow me on Medium. If you would like to take a look at the Jupyter notebook for this analysis please look <a href=\"https://github.com/raghav96/datascience/blob/master/Toxic%20Comment%20Dataset%20Challenge.ipynb\">here</a>. I\u2019m interested in collaborating to work on more data science projects! Please contact me if you would like to work any interesting datasets with me. I look forward to your opinion and comments.</p>\n<p><strong>References</strong></p>\n<ol>\n<li><a href=\"https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\">https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/</a></li>\n<li><a href=\"https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline\">https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline</a></li>\n<li><a href=\"https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras\">https://www.kaggle.com/sbongo/for-beginners-tackling-toxic-using-keras</a></li>\n</ol>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=3d5a12b06f50\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["machine-learning","towards-data-science","data-science"]},{"title":"House Prices in Ames, Iowa \u2014 Working with Kaggle","pubDate":"2018-07-31 00:15:15","link":"https://raghavism.medium.com/house-prices-in-ames-iowa-working-with-kaggle-91656d5ef6e0?source=rss-226ffefb5260------2","guid":"https://medium.com/p/91656d5ef6e0","author":"Raghav Ravisankar","thumbnail":"","description":"\n<p>This summer kicked off my interest with Data Science, starting of a journey of self-learning and exploration.</p>\n<p>House prices are an interesting dataset to look at because they usually have a lot of features in them and not all of the columns and values have a direct correlation to the price of the house. House prices depend on a lot of factors, mostly because of the general variation of preferences across the population, one thing however that is trivial for us to understand that bigger means more expensive. That is why house prices datasets are interesting to analyze and visualize, even more so, building a predictor to the prices of the house from the features of the house. This makes the dataset good to work with while learning the art of data\u00a0science.</p>\n<p>You can find my Jupyter notebook for the data analysis <a href=\"https://github.com/raghav96/datascience/blob/master/House%20Prices%20Dataset.ipynb\">here</a> and the dataset\u00a0<a href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\">here</a>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*iBFbeHjlLLtb4Eh3\"><figcaption>\u201cA large suburban house with a garden on a sunny day\u201d by <a href=\"https://unsplash.com/@jesseroberts?utm_source=medium&amp;utm_medium=referral\">Jesse Roberts</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Here I am using a very structured way to do this data analysis, and here are the steps I am taking. Each step will have a heading in this\u00a0article.</p>\n<ul>\n<li>Question or problem definition.</li>\n<li>Acquire training and testing\u00a0data.</li>\n<li>Wrangle, prepare, cleanse the\u00a0data.</li>\n<li>Analyze, identify patterns, and explore the\u00a0data.</li>\n<li>Model, predict and solve the\u00a0problem.</li>\n<li>Visualize, report, and present the problem solving steps and final solution.</li>\n<li>Supply or submit the\u00a0results.</li>\n</ul>\n<h3><strong>Question or problem definition</strong></h3>\n<p>So of to Kaggle, where there is this amazing <a href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\">dataset</a> is located. In this dataset, we have a list of house prices and information about the houses themselves from Ames, Iowa. Since it is a very descriptive dataset, there are a lot of things we can do with this dataset. However, to keep it short and sweet, let us define the scope of this analysis to be to complete a comprehensive exploratory data analysis (EDA) of the house prices and to build a regressor model of the prices of the houses based on the columns in the training set to predict house prices in the test set/validation set.</p>\n<p>Before you get started with this notebook, it is highly recommended to go through this <a href=\"https://ww2.amstat.org/publications/jse/v19n3/decock.pdf\">paper</a> from the author of the dataset to get an overview of the dataset and how the usage is intended. This notebook uses techniques used by Alexandru Papiu in his Jupyter <a href=\"https://www.kaggle.com/apapiu/regularized-linear-models\">notebook</a>. I figured learning how to approach a dataset with some help from the community is a good\u00a0start.</p>\n<h3>Acquire training and testing\u00a0data</h3>\n<p>Now let us get right down to it. Let us import all the required packages and training and test set first, and display the structure of the dataset using the shape and head functions</p>\n<pre><strong>import</strong> <strong>pandas</strong> <strong>as</strong> <strong>pd</strong><br><strong>import</strong> <strong>numpy</strong> <strong>as</strong> <strong>np</strong><br><strong>import</strong> <strong>matplotlib.pyplot</strong> <strong>as</strong> <strong>plt</strong><br><strong>import</strong> <strong>seaborn</strong> <strong>as</strong> <strong>sns</strong><br><strong>from</strong> <strong>matplotlib.colors</strong> <strong>import</strong> LogNorm<br><strong>from</strong> <strong>scipy.stats</strong> <strong>import</strong> skew<br><strong>from</strong> <strong>sklearn.model_selection</strong> <strong>import</strong> train_test_split<br><br>%matplotlib inline</pre>\n<pre>df_train = pd.read_csv('house-prices/train.csv')<br>df_test = pd.read_csv('house-prices/test.csv')</pre>\n<pre>df_train.shape<br># Output : (1460, 81)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4KqTZhQdLxdrTat9R9Uofw.png\"><figcaption>This is the how the dataset looks like returned after the calling df_train.head(5)</figcaption></figure><p>Here are the columns in the dataset, that we find out using this\u00a0command.</p>\n<pre>print(df_train.columns)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UuJbOwPao-kPJaCtrVBBtQ.png\"><figcaption>These are the columns that we get from the command df_train.columns</figcaption></figure><p>Now that we are aware of the structure of the dataset, let us take a closer look at the dataset to analyze the features. This brings us to the next step, wrangling, prepare and cleanse the\u00a0data.</p>\n<h3>Wrangle, prepare, cleanse the\u00a0data</h3>\n<p>In this section we are going to go through multiple techniques used to wrangle, prepare, cleanse the\u00a0data.</p>\n<p>Here is a breakdown of the techniques we are going to\u00a0use:</p>\n<ul>\n<li>Finding out if there are any duplicate entries in the\u00a0dataset</li>\n<li>Finding out about the type of columns: numeric and categorical</li>\n<li>Finding out the missing data in each\u00a0column</li>\n<li>Filling in missing values with relevant\u00a0data</li>\n<li>Finding and removing outliers from the\u00a0data</li>\n</ul>\n<p>Lets us get started right\u00a0away!</p>\n<ul><li><strong>Finding out if there are any duplicate entries in the\u00a0dataset</strong></li></ul>\n<p>Duplicate entries affect the accuracy of the predictor we intend to build, as the features of a specific house will affect the predictions of the classifier if there are duplicate entries of it. Here is the code to check if the data has duplicate columns.</p>\n<pre>uniqueRows = len(set(df_train.Id))<br>totalRows = len(df_train.Id)<br>duplicateRows = totalRows - uniqueRows<br><br><em># Tip: Assert allows you to test simple stuff in your data analysis as you go/sanity checks</em><br><strong>assert</strong> (duplicateRows==0)</pre>\n<ul><li>Finding out about the type of columns: numeric and categorical</li></ul>\n<p>Now we have to see which columns have numerical columns and which columns have categorical columns in order to build features from each column. This would be used later in order to do normalization and fixing the numerical columns in the\u00a0dataset.</p>\n<pre>numeric = [col <strong>for</strong> col <strong>in</strong> df_test.columns <strong>if</strong> df_test.dtypes[col] != 'object']<br>categorical = [col <strong>for</strong> col <strong>in</strong> df_test.columns <strong>if</strong> df_test.dtypes[col] == 'object']<br>print(numeric)<br>print(categorical)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*GUc7a1FwI9beDJh0TcdZMA.png\"><figcaption>These are the numerical and categorical columns in the\u00a0dataset</figcaption></figure><ul><li><strong>Finding out the missing data in each\u00a0column</strong></li></ul>\n<p>Missing columns in the data can either signify lack of data or a specific negative value within a categorical column of the\u00a0data.</p>\n<p>Now we identify the columns in the dataset that are missing, and to find out to find out the missing data for each column, we would be using this piece of code to find out the number of missing rows in each\u00a0column.</p>\n<pre><em># Finding out the columns that are missing values in the dataset</em><br>missing = df_train.isnull().sum()<br>missing = missing[missing &gt; 0]<br>missing.sort_values(inplace=<strong>True</strong>)<br>missing.plot.bar()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/810/1*oRzdHv3l5TUQzCUDfbX4qg.png\"><figcaption>The Y-axis here is the number of missing entries for each column in the\u00a0x-axis.</figcaption></figure><p>We can clearly see the columns PoolQC, MiscFeature, Alley, Fence have a lot of values that are missing. But taking a closer look at the data, we understand the some part of each of these columns have information regarding the Alley or PoolQC and Fence. We will be filling in the data with the mean value of each of the columns later during the analysis.</p>\n<ul><li><strong>Filling in missing values with relevant\u00a0data</strong></li></ul>\n<p>We will be doing this later when preparing the data for the model. Now let us move on to removing the outliers in the\u00a0data</p>\n<ul><li><strong>Finding and removing outliers from the\u00a0data</strong></li></ul>\n<p>Here we are trying to identify the outliers in the data which we cannot use in the model because it skews the analysis towards values that are unlikely. Here we intend to visualize the data to find out the spread of the data so we can remove the outliers in the\u00a0data.</p>\n<pre><em># Looking for outliers</em><br>plt.scatter(df_train.GrLivArea, df_train.SalePrice, color='red', marker='s')<br>plt.title('Data with outliers')<br>plt.xlabel('Living Area')<br>plt.ylabel('Sale Price')<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/952/1*KDP3ntzM9nJ-4CQqJESzmw.png\"><figcaption>This is the entire dataset with all the outliers in the\u00a0data.</figcaption></figure><p>Here we are removing houses from the dataset that are larger than 4000 sqft and more than 700,000 in value because they look like outliers in the\u00a0dataset.</p>\n<pre><em># Cleaning the dataset so there are no outliers in the dataset</em><br>df_train = df_train[(df_train.GrLivArea&lt;4000) &amp; (df_train.SalePrice&lt;700000)]<br>df_test = df_test[:df_train.shape[0]]<br>df_test.shape, df_train.shape</pre>\n<pre>plt.scatter(df_train.GrLivArea, df_train.SalePrice, color='blue', marker='s')<br>plt.title('Clean data without outliers')<br>plt.xlabel('Living Area')<br>plt.ylabel('Sale Price')plt.scatter(df_train.GrLivArea, df_train.SalePrice, color='blue', marker='s')<br>plt.title('Clean data without outliers')<br>plt.xlabel('Living Area')<br>plt.ylabel('Sale Price')</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/964/1*c8gr7MND_rQ8hiX97HO0AQ.png\"><figcaption>Here is the data without the\u00a0outliers</figcaption></figure><h3>Analyze, identify patterns, and explore the\u00a0data.</h3>\n<p><strong>Finding out the distribution of data within the\u00a0dataset</strong></p>\n<p>Now to find out how each of the variables are distributed amongst the dataset, we can pivot the table into its constituent features and find out how each variable is distributed amongst the dataset using pd.melt <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html\">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html</a></p>\n<pre>(eg) A B  -&gt; A variable value<br>     1 2     1 B        2<br>     1 3     1 B        3<br>     2 1     2 B        1<br>     2 2     2 B        2</pre>\n<p>For this we can only use numeric features and not any categorical features because of the nature of sns.FacetGrid module we are going to use to find correlations between the\u00a0dataset.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5mDnj0smoer5YF6H8ljoUw.png\"><figcaption>Distribution of various columns in the dataset and if we can model the data. Cleaner curves with rectangles within them are more accurately modeled.</figcaption></figure><p><strong>House prices over the\u00a0years</strong></p>\n<p>Now let us analyze the house prices over the course of the years and visualize this on a histogram. In this graph, each x column would represent the year and y-axis would represent the house prices for the\u00a0year.</p>\n<pre><em>#Across the recession in 2008</em><br>fig, ax = plt.subplots()<br>x= df_train.YrSold<br>y= df_train.SalePrice<br>counts, xedges, yedges, im = ax.hist2d(x, y, bins=40, norm=LogNorm())<br>plt.colorbar(im, ax=ax)<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/794/1*03t-Ki5HAWeZCJ9D80h5Qw.png\"><figcaption>Prices on the y-axis and year of sale on the\u00a0x-axis.</figcaption></figure><p><strong>Overall condition of the\u00a0house</strong></p>\n<p>Now let us plot the correlation between three columns: Year the house was sold, Overall condition of house and Living Area in the house. I wanted to visualize these leads in a seaborn pairplot so I used the following code to implement this.</p>\n<pre><strong>def</strong> cool_plot(df):<br>    snsplot = sns.pairplot(df_train[['SalePrice', 'GrLivArea','OverallCond']], hue='OverallCond', palette='husl', size=5);<br><br>    fig.set_size_inches(8, 10)<br><br>    plt.show()<br><br>cool_plot(df_train)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*d8yuligym3ElHFca-fsuuA.png\"><figcaption>A nice complex graph to find out the relation between these three\u00a0columns</figcaption></figure><p><strong>Correlation matrix</strong></p>\n<p>Now let us plot the correlation matrix for each of the columns in the dataset. This is done to have a clear picture on the correlation between columns in the\u00a0dataset.</p>\n<pre><em>#correlation matrix</em><br>corrmat = df_train.corr()<br>f, ax = plt.subplots(figsize=(12, 9))<br>sns.heatmap(corrmat,vmax=.8, square=<strong>True</strong>);</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*pwHPINODLGx6dwKY3-q1Ag.png\"><figcaption>Correlation matrix for the dataset, the darker the color, the more the correlation between the two\u00a0columns.</figcaption></figure><p><strong>Normalization</strong></p>\n<p>To model and predict the problem, we first need to understand that our data by itself has to be represented in a form that is easy for the predictor to learn from, hence we have to prepare the data; normalize the numerical columns in the data and encode the categorical columns in the dataset, in order to build an effective model for the problem. This warrants a detailed explanation, and is based on the ability of the algorithm to work better given the need of the algorithm minimize the distance of the data point from the classifier\u2019s prediction. This is also based on the ability of the computer to be able to deal with computation of smaller numbers more easily because of their representation.</p>\n<p>One example of normalization is using a log function: Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally. Notice the x-axis in the following graphs, they indicate the range of values taken up by the column. This following code show us how to get the\u00a0graph.</p>\n<pre><em># Increases the size of the histogram to fit both normalized and raw data</em><br>plt.rcParams['figure.figsize'] = (12.0, 6.0)<br>prices = pd.DataFrame({'log_prices':np.log1p(df_train[\"SalePrice\"]), 'prices': df_train[\"SalePrice\"]})<br>prices.hist()<br>plt.ylabel('Number of occurences')</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Y02DIGPzuUgv96He2FBXCQ.png\"><figcaption>After and before of taking the log of the prices\u00a0column</figcaption></figure><p>We take the log of the SalePrice in order to build a normalized model of the data, the reason why we do this is because predictors work better when the data is not distributed sparsely.</p>\n<p>In this we are going to use the skew function to find out the columns in which the numerical data is not too skewed\u00a0here.</p>\n<p>I learned how to do this from <a href=\"https://www.kaggle.com/apapiu/regularized-linear-models\">https://www.kaggle.com/apapiu/regularized-linear-models</a> where the skew function essentially returns the skewedness of the distribution of values in a column. We do not want columns with too much of skewedness because that affects the value returned by the log function.</p>\n<p>In the following steps we are going to compute the skewedness and normalize the numerical columns in the\u00a0data</p>\n<pre>df_train['SalePrice'] = np.log1p(df_train['SalePrice'])</pre>\n<pre>#log transform skewed numeric features:<br>skewed_feats = df_train[numeric].apply(lambda x: skew(x.dropna())) #compute skewness<br>skewed_feats = skewed_feats[skewed_feats &gt; 0.75]<br>skewed_feats = skewed_feats.index</pre>\n<pre>data[skewed_feats] = np.log1p(data[skewed_feats])</pre>\n<pre>df_train.shape, df_test.shape<br># ((1456, 81), (1456, 80))</pre>\n<p>Now let us encode the categorical columns in the data with the following code. This is done in order to convert the categorical columns into a feature based on the values in the dataset <em>pd.get_dummies() </em>takes a data of the form [a, b, b, c, null] and converts it into [0, 1, 1\u00a0, 2, 3]. This function has both the property of categorizing each value in the dataset and filling in the null\u00a0values.</p>\n<pre>print(\"NAs for features in train : \" + str(data.isnull().values.sum()))<br>data = pd.get_dummies(data)<br>data = data.fillna(data.mean())<br>print(\"Remaining NAs for features in train : \" + str(data.isnull().values.sum()))</pre>\n<pre># NAs for features in train : 13936<br># Remaining NAs for features in train : 0</pre>\n<p>Now let us make the test and training set from the normalized data that we will use for fitting the model and predictions.</p>\n<pre><em>#creating matrices for sklearn:</em><br>X_train = data[:df_train.shape[0]]<br>X_test = data[df_train.shape[0]:]<br>y = df_train.SalePrice</pre>\n<h3>Model, predict and solve the\u00a0problem</h3>\n<p>In this analysis we are going to try implement regression models from sklearn to try to fit this data. We can find more about these basic regression models in the <a href=\"https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\">link</a> which has a detailed explanation for each model or look at sklearn <a href=\"http://scikit-learn.org/stable/modules/linear_model.html\">documentation</a></p>\n<p>In this analysis we would using the RMSE value to find out the error of the model for a given set of the data. The root mean square error (RMSE) of the dataset is the square root of sum of the squared distance between the predictions of the classifier and the actual value in the dataset. In simple terms, Root mean square error for a dataset is a way to measure accuracy of the classifier with respect to the dataset. Closer to 1, worse the model. Closer to 0 better the\u00a0model.</p>\n<p>Here we are going to define the RMSE on the training set so we can train our classifiers on the training\u00a0set.</p>\n<pre><strong>from</strong> <strong>sklearn.linear_model</strong> <strong>import</strong> Ridge, Lasso, RidgeCV, LassoCV<br><strong>from</strong> <strong>sklearn.model_selection</strong> <strong>import</strong> cross_val_score<br><br><strong>def</strong> rmse_cv(model):<br>    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring = \"neg_mean_squared_error\", cv = 10))<br><strong>return</strong>(rmse)</pre>\n<p><strong>Ridge Regression</strong></p>\n<p>Now let us train our models on a Ridge regressor from <em>sklearn.linear_model</em> package. Ridge regularization has a parameter <em>alpha </em>which is used to tune the data to fit the model to the training or validation data. In this code below we are going to train the Ridge regressor and store the RMSE value in <em>cv_ridge_train</em></p>\n<pre>ridge_model = Ridge()</pre>\n<pre># Alpha is the regularization parameter<br>alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]<br>cv_ridge_train = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]</pre>\n<p>Now let us plot the values of <em>cv_ridge_train </em>in order to find out the <em>alpha </em>for which we will have the lowest\u00a0RMSE</p>\n<pre># Plotting the least mean square error on the training data<br>cv_ridge_train = pd.Series(cv_ridge_train, index = alphas)<br>cv_ridge_train.plot(title = \"Finding out the best regularization parameter\")<br>#cv_ridge_test = pd.Series(cv_ridge_test, index = alphas)<br>#cv_ridge_test.plot()<br>plt.xlabel(\"alpha\")<br>plt.ylabel(\"rmse\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Bo8M07rbDKknS7Y53ps0NQ.png\"><figcaption>Finding out the best regularization parameter, here its probably\u00a05.</figcaption></figure><p>Now let us find out the lowest RMSE for the Ridge regressor.</p>\n<pre>cv_ridge_train.min()<br># 0.11004105920734367</pre>\n<p>So the RMSE is about 0.110, which is pretty\u00a0good.</p>\n<p><strong>Lasso Regression</strong></p>\n<p>Implementing a lasso regressor is easy, this regressor works by training on columns that are relevant to the dataset and does not include data from columns that are not\u00a0needed.</p>\n<pre>model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)<br>rmse_cv(model_lasso).mean()<br># 0.10837242935531295</pre>\n<p>The RMSE is 0.108 which is lesser than the Ridge regressor model. The following code allows to me to find out the number of features that the Lasso model used for training and the columns its discarded.</p>\n<pre>coef = pd.Series(model_lasso.coef_, index = X_train.columns)<br>print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")</pre>\n<pre># Lasso picked 105 variables and eliminated the other 182 variables</pre>\n<p>Lasso picked 105 variables and eliminated the other 182 variables. Nice!</p>\n<p><strong>XGBoost model</strong></p>\n<p>Here we are going to try out the XGBoost model which is created as a package which implements the Gradient Boosting Regression. Here we use the XGBoost as a blackbox with some parameters. <a href=\"http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\">Here</a> you can find a detailed explanation on how this model works. Here is the <a href=\"https://xgboost.readthedocs.io/en/latest/\">documentation</a> for this\u00a0package.</p>\n<p>The following code is going to give us the RMSE for the training and test set for XGBoost model and plot the\u00a0data.</p>\n<pre><strong>import</strong> <strong>xgboost</strong> <strong>as</strong> <strong>xgb</strong></pre>\n<pre>dtrain = xgb.DMatrix(X_train, label = y)<br>dtest = xgb.DMatrix(X_test)<br><br>params = {\"max_depth\":2, \"eta\":0.1}<br>model = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)</pre>\n<pre>model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*AOiTtQbMtt0lOo8707U33A.png\"><figcaption>Blue is the RMSE mean for the test set, Orange is the RMSE mean for the training set. Y-axis is the value of RMSE and x-axis signifies the number of epochs in training.</figcaption></figure><p>The following code below is going to fit the XGBoost model on the training\u00a0data</p>\n<pre>model_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) <em>#the params were tuned using xgb.cv</em><br>model_xgb.fit(X_train, y)</pre>\n<p>The following code is going to predict values from the Lasso model and the XGBoost model and convert them back to normal after normalization.</p>\n<pre>xgb_preds = np.expm1(model_xgb.predict(X_test))<br>lasso_preds = np.expm1(model_lasso.predict(X_test))</pre>\n<p>The following code shows the correlation between the predictions of the two regressors, showing that the predictions from the two regressors have a\u00a0pattern.</p>\n<pre>predictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})<br>predictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tMlRjenhNE2UDmcdPO75Fg.png\"><figcaption>Correlation between predictions on the XGB model and predictions on the Lasso\u00a0model.</figcaption></figure><p>Now using the predictions from two different predictors, we can combine the models using a simple linear\u00a0function</p>\n<pre>preds = 0.7*lasso_preds + 0.3*xgb_preds</pre>\n<h3><strong>Supply or submit the\u00a0results.</strong></h3>\n<p>Now we are going to export the data into a csv file to supply and submit the predictions</p>\n<pre>solution = pd.DataFrame({\"id\":df_test.Id, \"SalePrice\":preds})<br>solution.to_csv(\"ridge.csv\", index = False)</pre>\n<h3>The end!</h3>\n<p>Thanks for reading my article, the code for this analysis is available <a href=\"https://github.com/raghav96/datascience/blob/master/House%20Prices%20Dataset.ipynb\">here</a>. You can follow me on Medium at <a href=\"https://medium.com/u/226ffefb5260\">Raghav Ravisankar</a>. Please let me know your feedback through the comments!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=91656d5ef6e0\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>This summer kicked off my interest with Data Science, starting of a journey of self-learning and exploration.</p>\n<p>House prices are an interesting dataset to look at because they usually have a lot of features in them and not all of the columns and values have a direct correlation to the price of the house. House prices depend on a lot of factors, mostly because of the general variation of preferences across the population, one thing however that is trivial for us to understand that bigger means more expensive. That is why house prices datasets are interesting to analyze and visualize, even more so, building a predictor to the prices of the house from the features of the house. This makes the dataset good to work with while learning the art of data\u00a0science.</p>\n<p>You can find my Jupyter notebook for the data analysis <a href=\"https://github.com/raghav96/datascience/blob/master/House%20Prices%20Dataset.ipynb\">here</a> and the dataset\u00a0<a href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\">here</a>.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*iBFbeHjlLLtb4Eh3\"><figcaption>\u201cA large suburban house with a garden on a sunny day\u201d by <a href=\"https://unsplash.com/@jesseroberts?utm_source=medium&amp;utm_medium=referral\">Jesse Roberts</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>Here I am using a very structured way to do this data analysis, and here are the steps I am taking. Each step will have a heading in this\u00a0article.</p>\n<ul>\n<li>Question or problem definition.</li>\n<li>Acquire training and testing\u00a0data.</li>\n<li>Wrangle, prepare, cleanse the\u00a0data.</li>\n<li>Analyze, identify patterns, and explore the\u00a0data.</li>\n<li>Model, predict and solve the\u00a0problem.</li>\n<li>Visualize, report, and present the problem solving steps and final solution.</li>\n<li>Supply or submit the\u00a0results.</li>\n</ul>\n<h3><strong>Question or problem definition</strong></h3>\n<p>So of to Kaggle, where there is this amazing <a href=\"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\">dataset</a> is located. In this dataset, we have a list of house prices and information about the houses themselves from Ames, Iowa. Since it is a very descriptive dataset, there are a lot of things we can do with this dataset. However, to keep it short and sweet, let us define the scope of this analysis to be to complete a comprehensive exploratory data analysis (EDA) of the house prices and to build a regressor model of the prices of the houses based on the columns in the training set to predict house prices in the test set/validation set.</p>\n<p>Before you get started with this notebook, it is highly recommended to go through this <a href=\"https://ww2.amstat.org/publications/jse/v19n3/decock.pdf\">paper</a> from the author of the dataset to get an overview of the dataset and how the usage is intended. This notebook uses techniques used by Alexandru Papiu in his Jupyter <a href=\"https://www.kaggle.com/apapiu/regularized-linear-models\">notebook</a>. I figured learning how to approach a dataset with some help from the community is a good\u00a0start.</p>\n<h3>Acquire training and testing\u00a0data</h3>\n<p>Now let us get right down to it. Let us import all the required packages and training and test set first, and display the structure of the dataset using the shape and head functions</p>\n<pre><strong>import</strong> <strong>pandas</strong> <strong>as</strong> <strong>pd</strong><br><strong>import</strong> <strong>numpy</strong> <strong>as</strong> <strong>np</strong><br><strong>import</strong> <strong>matplotlib.pyplot</strong> <strong>as</strong> <strong>plt</strong><br><strong>import</strong> <strong>seaborn</strong> <strong>as</strong> <strong>sns</strong><br><strong>from</strong> <strong>matplotlib.colors</strong> <strong>import</strong> LogNorm<br><strong>from</strong> <strong>scipy.stats</strong> <strong>import</strong> skew<br><strong>from</strong> <strong>sklearn.model_selection</strong> <strong>import</strong> train_test_split<br><br>%matplotlib inline</pre>\n<pre>df_train = pd.read_csv('house-prices/train.csv')<br>df_test = pd.read_csv('house-prices/test.csv')</pre>\n<pre>df_train.shape<br># Output : (1460, 81)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4KqTZhQdLxdrTat9R9Uofw.png\"><figcaption>This is the how the dataset looks like returned after the calling df_train.head(5)</figcaption></figure><p>Here are the columns in the dataset, that we find out using this\u00a0command.</p>\n<pre>print(df_train.columns)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*UuJbOwPao-kPJaCtrVBBtQ.png\"><figcaption>These are the columns that we get from the command df_train.columns</figcaption></figure><p>Now that we are aware of the structure of the dataset, let us take a closer look at the dataset to analyze the features. This brings us to the next step, wrangling, prepare and cleanse the\u00a0data.</p>\n<h3>Wrangle, prepare, cleanse the\u00a0data</h3>\n<p>In this section we are going to go through multiple techniques used to wrangle, prepare, cleanse the\u00a0data.</p>\n<p>Here is a breakdown of the techniques we are going to\u00a0use:</p>\n<ul>\n<li>Finding out if there are any duplicate entries in the\u00a0dataset</li>\n<li>Finding out about the type of columns: numeric and categorical</li>\n<li>Finding out the missing data in each\u00a0column</li>\n<li>Filling in missing values with relevant\u00a0data</li>\n<li>Finding and removing outliers from the\u00a0data</li>\n</ul>\n<p>Lets us get started right\u00a0away!</p>\n<ul><li><strong>Finding out if there are any duplicate entries in the\u00a0dataset</strong></li></ul>\n<p>Duplicate entries affect the accuracy of the predictor we intend to build, as the features of a specific house will affect the predictions of the classifier if there are duplicate entries of it. Here is the code to check if the data has duplicate columns.</p>\n<pre>uniqueRows = len(set(df_train.Id))<br>totalRows = len(df_train.Id)<br>duplicateRows = totalRows - uniqueRows<br><br><em># Tip: Assert allows you to test simple stuff in your data analysis as you go/sanity checks</em><br><strong>assert</strong> (duplicateRows==0)</pre>\n<ul><li>Finding out about the type of columns: numeric and categorical</li></ul>\n<p>Now we have to see which columns have numerical columns and which columns have categorical columns in order to build features from each column. This would be used later in order to do normalization and fixing the numerical columns in the\u00a0dataset.</p>\n<pre>numeric = [col <strong>for</strong> col <strong>in</strong> df_test.columns <strong>if</strong> df_test.dtypes[col] != 'object']<br>categorical = [col <strong>for</strong> col <strong>in</strong> df_test.columns <strong>if</strong> df_test.dtypes[col] == 'object']<br>print(numeric)<br>print(categorical)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*GUc7a1FwI9beDJh0TcdZMA.png\"><figcaption>These are the numerical and categorical columns in the\u00a0dataset</figcaption></figure><ul><li><strong>Finding out the missing data in each\u00a0column</strong></li></ul>\n<p>Missing columns in the data can either signify lack of data or a specific negative value within a categorical column of the\u00a0data.</p>\n<p>Now we identify the columns in the dataset that are missing, and to find out to find out the missing data for each column, we would be using this piece of code to find out the number of missing rows in each\u00a0column.</p>\n<pre><em># Finding out the columns that are missing values in the dataset</em><br>missing = df_train.isnull().sum()<br>missing = missing[missing &gt; 0]<br>missing.sort_values(inplace=<strong>True</strong>)<br>missing.plot.bar()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/810/1*oRzdHv3l5TUQzCUDfbX4qg.png\"><figcaption>The Y-axis here is the number of missing entries for each column in the\u00a0x-axis.</figcaption></figure><p>We can clearly see the columns PoolQC, MiscFeature, Alley, Fence have a lot of values that are missing. But taking a closer look at the data, we understand the some part of each of these columns have information regarding the Alley or PoolQC and Fence. We will be filling in the data with the mean value of each of the columns later during the analysis.</p>\n<ul><li><strong>Filling in missing values with relevant\u00a0data</strong></li></ul>\n<p>We will be doing this later when preparing the data for the model. Now let us move on to removing the outliers in the\u00a0data</p>\n<ul><li><strong>Finding and removing outliers from the\u00a0data</strong></li></ul>\n<p>Here we are trying to identify the outliers in the data which we cannot use in the model because it skews the analysis towards values that are unlikely. Here we intend to visualize the data to find out the spread of the data so we can remove the outliers in the\u00a0data.</p>\n<pre><em># Looking for outliers</em><br>plt.scatter(df_train.GrLivArea, df_train.SalePrice, color='red', marker='s')<br>plt.title('Data with outliers')<br>plt.xlabel('Living Area')<br>plt.ylabel('Sale Price')<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/952/1*KDP3ntzM9nJ-4CQqJESzmw.png\"><figcaption>This is the entire dataset with all the outliers in the\u00a0data.</figcaption></figure><p>Here we are removing houses from the dataset that are larger than 4000 sqft and more than 700,000 in value because they look like outliers in the\u00a0dataset.</p>\n<pre><em># Cleaning the dataset so there are no outliers in the dataset</em><br>df_train = df_train[(df_train.GrLivArea&lt;4000) &amp; (df_train.SalePrice&lt;700000)]<br>df_test = df_test[:df_train.shape[0]]<br>df_test.shape, df_train.shape</pre>\n<pre>plt.scatter(df_train.GrLivArea, df_train.SalePrice, color='blue', marker='s')<br>plt.title('Clean data without outliers')<br>plt.xlabel('Living Area')<br>plt.ylabel('Sale Price')plt.scatter(df_train.GrLivArea, df_train.SalePrice, color='blue', marker='s')<br>plt.title('Clean data without outliers')<br>plt.xlabel('Living Area')<br>plt.ylabel('Sale Price')</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/964/1*c8gr7MND_rQ8hiX97HO0AQ.png\"><figcaption>Here is the data without the\u00a0outliers</figcaption></figure><h3>Analyze, identify patterns, and explore the\u00a0data.</h3>\n<p><strong>Finding out the distribution of data within the\u00a0dataset</strong></p>\n<p>Now to find out how each of the variables are distributed amongst the dataset, we can pivot the table into its constituent features and find out how each variable is distributed amongst the dataset using pd.melt <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html\">http://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html</a></p>\n<pre>(eg) A B  -&gt; A variable value<br>     1 2     1 B        2<br>     1 3     1 B        3<br>     2 1     2 B        1<br>     2 2     2 B        2</pre>\n<p>For this we can only use numeric features and not any categorical features because of the nature of sns.FacetGrid module we are going to use to find correlations between the\u00a0dataset.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*5mDnj0smoer5YF6H8ljoUw.png\"><figcaption>Distribution of various columns in the dataset and if we can model the data. Cleaner curves with rectangles within them are more accurately modeled.</figcaption></figure><p><strong>House prices over the\u00a0years</strong></p>\n<p>Now let us analyze the house prices over the course of the years and visualize this on a histogram. In this graph, each x column would represent the year and y-axis would represent the house prices for the\u00a0year.</p>\n<pre><em>#Across the recession in 2008</em><br>fig, ax = plt.subplots()<br>x= df_train.YrSold<br>y= df_train.SalePrice<br>counts, xedges, yedges, im = ax.hist2d(x, y, bins=40, norm=LogNorm())<br>plt.colorbar(im, ax=ax)<br>plt.show()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/794/1*03t-Ki5HAWeZCJ9D80h5Qw.png\"><figcaption>Prices on the y-axis and year of sale on the\u00a0x-axis.</figcaption></figure><p><strong>Overall condition of the\u00a0house</strong></p>\n<p>Now let us plot the correlation between three columns: Year the house was sold, Overall condition of house and Living Area in the house. I wanted to visualize these leads in a seaborn pairplot so I used the following code to implement this.</p>\n<pre><strong>def</strong> cool_plot(df):<br>    snsplot = sns.pairplot(df_train[['SalePrice', 'GrLivArea','OverallCond']], hue='OverallCond', palette='husl', size=5);<br><br>    fig.set_size_inches(8, 10)<br><br>    plt.show()<br><br>cool_plot(df_train)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*d8yuligym3ElHFca-fsuuA.png\"><figcaption>A nice complex graph to find out the relation between these three\u00a0columns</figcaption></figure><p><strong>Correlation matrix</strong></p>\n<p>Now let us plot the correlation matrix for each of the columns in the dataset. This is done to have a clear picture on the correlation between columns in the\u00a0dataset.</p>\n<pre><em>#correlation matrix</em><br>corrmat = df_train.corr()<br>f, ax = plt.subplots(figsize=(12, 9))<br>sns.heatmap(corrmat,vmax=.8, square=<strong>True</strong>);</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*pwHPINODLGx6dwKY3-q1Ag.png\"><figcaption>Correlation matrix for the dataset, the darker the color, the more the correlation between the two\u00a0columns.</figcaption></figure><p><strong>Normalization</strong></p>\n<p>To model and predict the problem, we first need to understand that our data by itself has to be represented in a form that is easy for the predictor to learn from, hence we have to prepare the data; normalize the numerical columns in the data and encode the categorical columns in the dataset, in order to build an effective model for the problem. This warrants a detailed explanation, and is based on the ability of the algorithm to work better given the need of the algorithm minimize the distance of the data point from the classifier\u2019s prediction. This is also based on the ability of the computer to be able to deal with computation of smaller numbers more easily because of their representation.</p>\n<p>One example of normalization is using a log function: Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally. Notice the x-axis in the following graphs, they indicate the range of values taken up by the column. This following code show us how to get the\u00a0graph.</p>\n<pre><em># Increases the size of the histogram to fit both normalized and raw data</em><br>plt.rcParams['figure.figsize'] = (12.0, 6.0)<br>prices = pd.DataFrame({'log_prices':np.log1p(df_train[\"SalePrice\"]), 'prices': df_train[\"SalePrice\"]})<br>prices.hist()<br>plt.ylabel('Number of occurences')</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Y02DIGPzuUgv96He2FBXCQ.png\"><figcaption>After and before of taking the log of the prices\u00a0column</figcaption></figure><p>We take the log of the SalePrice in order to build a normalized model of the data, the reason why we do this is because predictors work better when the data is not distributed sparsely.</p>\n<p>In this we are going to use the skew function to find out the columns in which the numerical data is not too skewed\u00a0here.</p>\n<p>I learned how to do this from <a href=\"https://www.kaggle.com/apapiu/regularized-linear-models\">https://www.kaggle.com/apapiu/regularized-linear-models</a> where the skew function essentially returns the skewedness of the distribution of values in a column. We do not want columns with too much of skewedness because that affects the value returned by the log function.</p>\n<p>In the following steps we are going to compute the skewedness and normalize the numerical columns in the\u00a0data</p>\n<pre>df_train['SalePrice'] = np.log1p(df_train['SalePrice'])</pre>\n<pre>#log transform skewed numeric features:<br>skewed_feats = df_train[numeric].apply(lambda x: skew(x.dropna())) #compute skewness<br>skewed_feats = skewed_feats[skewed_feats &gt; 0.75]<br>skewed_feats = skewed_feats.index</pre>\n<pre>data[skewed_feats] = np.log1p(data[skewed_feats])</pre>\n<pre>df_train.shape, df_test.shape<br># ((1456, 81), (1456, 80))</pre>\n<p>Now let us encode the categorical columns in the data with the following code. This is done in order to convert the categorical columns into a feature based on the values in the dataset <em>pd.get_dummies() </em>takes a data of the form [a, b, b, c, null] and converts it into [0, 1, 1\u00a0, 2, 3]. This function has both the property of categorizing each value in the dataset and filling in the null\u00a0values.</p>\n<pre>print(\"NAs for features in train : \" + str(data.isnull().values.sum()))<br>data = pd.get_dummies(data)<br>data = data.fillna(data.mean())<br>print(\"Remaining NAs for features in train : \" + str(data.isnull().values.sum()))</pre>\n<pre># NAs for features in train : 13936<br># Remaining NAs for features in train : 0</pre>\n<p>Now let us make the test and training set from the normalized data that we will use for fitting the model and predictions.</p>\n<pre><em>#creating matrices for sklearn:</em><br>X_train = data[:df_train.shape[0]]<br>X_test = data[df_train.shape[0]:]<br>y = df_train.SalePrice</pre>\n<h3>Model, predict and solve the\u00a0problem</h3>\n<p>In this analysis we are going to try implement regression models from sklearn to try to fit this data. We can find more about these basic regression models in the <a href=\"https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/\">link</a> which has a detailed explanation for each model or look at sklearn <a href=\"http://scikit-learn.org/stable/modules/linear_model.html\">documentation</a></p>\n<p>In this analysis we would using the RMSE value to find out the error of the model for a given set of the data. The root mean square error (RMSE) of the dataset is the square root of sum of the squared distance between the predictions of the classifier and the actual value in the dataset. In simple terms, Root mean square error for a dataset is a way to measure accuracy of the classifier with respect to the dataset. Closer to 1, worse the model. Closer to 0 better the\u00a0model.</p>\n<p>Here we are going to define the RMSE on the training set so we can train our classifiers on the training\u00a0set.</p>\n<pre><strong>from</strong> <strong>sklearn.linear_model</strong> <strong>import</strong> Ridge, Lasso, RidgeCV, LassoCV<br><strong>from</strong> <strong>sklearn.model_selection</strong> <strong>import</strong> cross_val_score<br><br><strong>def</strong> rmse_cv(model):<br>    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring = \"neg_mean_squared_error\", cv = 10))<br><strong>return</strong>(rmse)</pre>\n<p><strong>Ridge Regression</strong></p>\n<p>Now let us train our models on a Ridge regressor from <em>sklearn.linear_model</em> package. Ridge regularization has a parameter <em>alpha </em>which is used to tune the data to fit the model to the training or validation data. In this code below we are going to train the Ridge regressor and store the RMSE value in <em>cv_ridge_train</em></p>\n<pre>ridge_model = Ridge()</pre>\n<pre># Alpha is the regularization parameter<br>alphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]<br>cv_ridge_train = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]</pre>\n<p>Now let us plot the values of <em>cv_ridge_train </em>in order to find out the <em>alpha </em>for which we will have the lowest\u00a0RMSE</p>\n<pre># Plotting the least mean square error on the training data<br>cv_ridge_train = pd.Series(cv_ridge_train, index = alphas)<br>cv_ridge_train.plot(title = \"Finding out the best regularization parameter\")<br>#cv_ridge_test = pd.Series(cv_ridge_test, index = alphas)<br>#cv_ridge_test.plot()<br>plt.xlabel(\"alpha\")<br>plt.ylabel(\"rmse\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Bo8M07rbDKknS7Y53ps0NQ.png\"><figcaption>Finding out the best regularization parameter, here its probably\u00a05.</figcaption></figure><p>Now let us find out the lowest RMSE for the Ridge regressor.</p>\n<pre>cv_ridge_train.min()<br># 0.11004105920734367</pre>\n<p>So the RMSE is about 0.110, which is pretty\u00a0good.</p>\n<p><strong>Lasso Regression</strong></p>\n<p>Implementing a lasso regressor is easy, this regressor works by training on columns that are relevant to the dataset and does not include data from columns that are not\u00a0needed.</p>\n<pre>model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)<br>rmse_cv(model_lasso).mean()<br># 0.10837242935531295</pre>\n<p>The RMSE is 0.108 which is lesser than the Ridge regressor model. The following code allows to me to find out the number of features that the Lasso model used for training and the columns its discarded.</p>\n<pre>coef = pd.Series(model_lasso.coef_, index = X_train.columns)<br>print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")</pre>\n<pre># Lasso picked 105 variables and eliminated the other 182 variables</pre>\n<p>Lasso picked 105 variables and eliminated the other 182 variables. Nice!</p>\n<p><strong>XGBoost model</strong></p>\n<p>Here we are going to try out the XGBoost model which is created as a package which implements the Gradient Boosting Regression. Here we use the XGBoost as a blackbox with some parameters. <a href=\"http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/\">Here</a> you can find a detailed explanation on how this model works. Here is the <a href=\"https://xgboost.readthedocs.io/en/latest/\">documentation</a> for this\u00a0package.</p>\n<p>The following code is going to give us the RMSE for the training and test set for XGBoost model and plot the\u00a0data.</p>\n<pre><strong>import</strong> <strong>xgboost</strong> <strong>as</strong> <strong>xgb</strong></pre>\n<pre>dtrain = xgb.DMatrix(X_train, label = y)<br>dtest = xgb.DMatrix(X_test)<br><br>params = {\"max_depth\":2, \"eta\":0.1}<br>model = xgb.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)</pre>\n<pre>model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*AOiTtQbMtt0lOo8707U33A.png\"><figcaption>Blue is the RMSE mean for the test set, Orange is the RMSE mean for the training set. Y-axis is the value of RMSE and x-axis signifies the number of epochs in training.</figcaption></figure><p>The following code below is going to fit the XGBoost model on the training\u00a0data</p>\n<pre>model_xgb = xgb.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) <em>#the params were tuned using xgb.cv</em><br>model_xgb.fit(X_train, y)</pre>\n<p>The following code is going to predict values from the Lasso model and the XGBoost model and convert them back to normal after normalization.</p>\n<pre>xgb_preds = np.expm1(model_xgb.predict(X_test))<br>lasso_preds = np.expm1(model_lasso.predict(X_test))</pre>\n<p>The following code shows the correlation between the predictions of the two regressors, showing that the predictions from the two regressors have a\u00a0pattern.</p>\n<pre>predictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})<br>predictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tMlRjenhNE2UDmcdPO75Fg.png\"><figcaption>Correlation between predictions on the XGB model and predictions on the Lasso\u00a0model.</figcaption></figure><p>Now using the predictions from two different predictors, we can combine the models using a simple linear\u00a0function</p>\n<pre>preds = 0.7*lasso_preds + 0.3*xgb_preds</pre>\n<h3><strong>Supply or submit the\u00a0results.</strong></h3>\n<p>Now we are going to export the data into a csv file to supply and submit the predictions</p>\n<pre>solution = pd.DataFrame({\"id\":df_test.Id, \"SalePrice\":preds})<br>solution.to_csv(\"ridge.csv\", index = False)</pre>\n<h3>The end!</h3>\n<p>Thanks for reading my article, the code for this analysis is available <a href=\"https://github.com/raghav96/datascience/blob/master/House%20Prices%20Dataset.ipynb\">here</a>. You can follow me on Medium at <a href=\"https://medium.com/u/226ffefb5260\">Raghav Ravisankar</a>. Please let me know your feedback through the comments!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=91656d5ef6e0\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-science","towards-data-science"]},{"title":"Exploring the Titanic Dataset","pubDate":"2018-07-14 01:11:03","link":"https://raghavism.medium.com/exploring-the-titanic-dataset-68e50263c14d?source=rss-226ffefb5260------2","guid":"https://medium.com/p/68e50263c14d","author":"Raghav Ravisankar","thumbnail":"","description":"\n<p>As the hot summer is upon us, I decided to venture out into a analysis of data from the ice cold depths of the Northern Atlantic Ocean. This article is moderately rigorous and goes through each step I have taken to build this analysis of the titanic dataset. I built this analysis with help from the <a href=\"https://www.kaggle.com/startupsci/titanic-data-science-solutions/\">Titanic Data Science Solutions</a> kernel. The dataset was obtained from Kaggle at this <a href=\"https://www.kaggle.com/c/titanic\">link</a>. Tldr; get the Jupyter notebook from this analysis\u00a0<a href=\"https://github.com/raghav96/datascience/blob/master/Titanic%20Dataset%20Kaggle%20Competition.ipynb\">here</a>.</p>\n<p>In the early hours of 15 April 1912, the RMS Titanic had sunk on collision with an iceberg in its maiden voyage from Southampton to New York City. There were an estimated 2224 passengers on board, and more than 1500 died, making it one of the worst passenger ship disasters in\u00a0history.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Y3d3HF03WSE6nMzj\"><figcaption>\u201cA cruise ship on a mountain lake with heavy clouds above\u201d by <a href=\"https://unsplash.com/@steinart?utm_source=medium&amp;utm_medium=referral\">Steinar Engeland</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>So given that, lets take a closer look at the people who survived this disaster and predict the likelihood of survival.</p>\n<p>For this analysis, I would be using python packages like pandas, seaborn and numpy and jupyter notebooks.</p>\n<p>Initially I loaded up the dataset onto the Jupyter notebook, using the command <em>pandas.read_csv(file)</em> and imported the data into the notebook. Then I proceeded to review the structure of the dataset to understand what features we can build our model\u00a0on.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*crP8o_KXj02q-zzpE8TFXg.png\"></figure><p>The variables that we had provided had the following descriptors to mean what they\u00a0were:</p>\n<ul>\n<li>Survived: Survived (1) or died\u00a0(0)</li>\n<li>Pclass: Passenger\u2019s class</li>\n<li>Name: Passenger\u2019s name</li>\n<li>Sex: Passenger\u2019s sex</li>\n<li>Age: Passenger\u2019s age</li>\n<li>SibSp: Number of siblings/spouses aboard</li>\n<li>Parch: Number of parents/children aboard</li>\n<li>Ticket: Ticket\u00a0number</li>\n<li>Fare: Fare</li>\n<li>Cabin: Cabin</li>\n<li>Embarked: Port of embarkation</li>\n</ul>\n<p>Trying to find out how much each feature contributes to the survival rate, I first tried to find out the expect survival for 4 features:</p>\n<ul>\n<li>Passenger Class or PClass\u200a\u2014\u200aUsually, higher class (class 1) meant a more expensive fare and the other classes paid lesser to be on the\u00a0ship</li>\n<li>Sex\u200a\u2014\u200aWhat was the survival rate of\u00a0women?</li>\n<li>SibSp\u200a\u2014\u200aWhat was the variation of survival rate based on number of siblings the passenger had on the\u00a0ship?</li>\n<li>Parch\u200a\u2014\u200aWhat was the variation of survival rate based on number of parents/children the passenger had on the\u00a0ship?</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qbCWQcpuArUOQLkfCLGmsQ.png\"><figcaption>Variation with class and gender, women made it alive on the Titanic way more than men because women and children were provided with lifeboats first when the Titanic was\u00a0sinking</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hlUV9v07NaComUBkCCDBfQ.png\"><figcaption>Percentages of survival for number of siblings and number of parents/children</figcaption></figure><p>Now, these percentages and numbers are likely to bore anyone. So lets make some plots and graphs on the survival rates. You can get the code for this on the notebook at the end of this\u00a0post</p>\n<p>So here\u2019s a plot of variation of survival amongst people of different age. The graph on the left indicates the number of people who did not survive and the graph on the right indicates the number of people who survived the Titanic disaster. As we can see from the graph on the right, infants and toddlers (ages 0\u20134) had a bigger survival rate compared to the children and teenagers (ages 5\u201316). Pretty sad stuff but the data doesn\u2019t lie or hide these things from\u00a0us.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/940/1*i3Hnp60OdUbs7XKarvcn6Q.png\"><figcaption>Variation of death or survival with\u00a0age</figcaption></figure><p>Now lets take a look at the variation of age amongst people who survived/did not survive based on the passenger class they were in. What is interesting in this data is that people who were in the last passenger class (class=3) and who were between the ages of 16\u201340 did not survive the Titanic sinking. So I guess Jack from the movie Titanic was statistically very likely to die in the disaster.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0A1B2Hv8fzkUkmEfuya9Wg.png\"><figcaption>Variation of survival and death with age amongst passengers of different passenger classes</figcaption></figure><p>Let\u2019s see how much these passengers paid to be on the Titanic and where they started the journey from. And if there is any correlation between the money they paid and surviving the Titanic disaster. So there seems to be no correlation of survival between fare paid and the likeliness or survival amongst the three different embarking cities.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4sM8L_P8tcXbpC0FC8lkvw.png\"></figure><p>So let\u2019s start the real analysis here. Here I am trying to build a predictive model based on the In this analysis I used different feature extraction techniques to build a encoding vector for the dataset I am provided. Firstly, I tried to convert a simple category into a binary feature. I took a simple one like Sex and converted the values into 0 for male and 1 for female in this\u00a0way.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JspPX9QNiaE7xHFL5OUWfg.png\"><figcaption>Converting the Sex column which was text into a binary 0 or 1 numerical column</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/334/1*iqUUHne68Dr3gOK3rbdPjw.png\"></figure><p>Then, I wanted to try a more complex feature from the names of the passengers. So I tried to extract the title from the names of the passengers and ended up with this on the right. Here I found out some of these titles were\u00a0similar:</p>\n<ul>\n<li>Mlle means \u2018Mademoiselle\u2019 which is the honorific used to describe someone who would be called \u2018Miss\u2019 in\u00a0France</li>\n<li>Mme means \u2018Madame\u2019 which is the honorific used to describe someone who would be called \u2018Mrs\u2019 in\u00a0France</li>\n<li>Ms is the short form for\u00a0\u2018Miss\u2019</li>\n<li>Lady, Countess, Dona are female honorifics of\u00a0nobility</li>\n<li>Don, Sir, Jonkheer are male honorifics of\u00a0nobility</li>\n<li>Capt refers to the Captain of the Titanic; Col, Major are military positions; Dr are doctors, Rev is a Reverend who all have special roles in\u00a0society</li>\n</ul>\n<p>Based on that, I came up with this list of titles and the chances of survival for each\u00a0title</p>\n<ul>\n<li>Master\u200a\u2014\u200aused to denote someone\u00a0younger</li>\n<li>Miss\u200a\u2014\u200ayoung\u00a0women</li>\n<li>Mr\u200a\u2014\u200amen</li>\n<li>Mrs\u200a\u2014\u200amarried\u00a0women</li>\n<li>Royalty\u200a\u2014\u200apeople with fancy\u00a0titles</li>\n<li>Special\u200a\u2014\u200athe captain, doctors and reverends who might have been called onto help during the\u00a0disaster</li>\n</ul>\n<p>On extracting them from the database and building a title feature we get survival rates based on each title. Looks like a lot of women survived, especially ones that were married. Titanic was a big widowmaker.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*T4i08wh_dzJCRM0ndZgw8g.png\"></figure><p>Now there is another thing we should look to do when we are building a predictive model, fill in values to fill columns with null values. Now let us analyze the Age column in order to fill null values with a predicted the value and build our feature from\u00a0that.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_nRnzIR5O-jpfAbnog_lYA.png\"><figcaption>There are a 177 rows with null values in the Age\u00a0column</figcaption></figure><p>So for filling in the null values in columns with predicted values, let us calculate the median age for passengers in each passenger class and sex; and fill in the age values for the 177 rows based on their passenger class and\u00a0sex.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lv8V5adGH22vR8IqTxCoIQ.png\"><figcaption>Calculating the median age by passenger class and\u00a0sex</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8yZnHUvll4jWeeF_SBNUwA.png\"><figcaption>Filling in the null values in the age column to get a complete Age\u00a0column</figcaption></figure><p>Now since we have filled up all the null values, the Age column has become a continuous range of numerical values. We could convert that into a feature by creating bands of values that we define. Here I chose my age groups as the following:</p>\n<ul>\n<li>Age 16 and\u00a0under</li>\n<li>Ages 16\u201332</li>\n<li>Ages 32\u201348</li>\n<li>Ages 48\u201364</li>\n<li>Ages 64 and\u00a0above.</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Nv9befZsIYfeDBKE01Pavw.png\"><figcaption>Building a feature based on Age\u00a0group</figcaption></figure><p>Now, I followed up to build a family size feature by adding the two columns that were provided to us to create a new column, SibSp\u200a\u2014\u200awhich denotes the number of siblings who were also passengers and Parch\u200a\u2014\u200awhich denotes the number of parents/children.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*GJhD3tdoHGQDMWeI1sHCNg.png\"><figcaption>FamilySize feature being built by combining the number of siblings and parents, sorted for survival rate. Looks like 4 member families are good for surviving sinking passenger ships.</figcaption></figure><p>So I built a couple more features like this, by making categorical columns into numeric features with the Embarked column and eventually ended up with this as my training and testing\u00a0data.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-L-rkc_-FbXMPGvpfigxtA.png\"><figcaption>Final cleaned and feature extracted training and test\u00a0dataset</figcaption></figure><p>Now, over to trying out different basic predictive models on the training and testing data. In order to build our analysis, I am going to use the following predictive models:</p>\n<ul>\n<li>Logistic Regression</li>\n<li>Support Vector\u00a0Machines</li>\n<li>KNN or K-Nearest Neighbors</li>\n<li>Decision Trees</li>\n<li>Random Forest</li>\n</ul>\n<p>Logistic Regression is useful to see which features affect the survival rate of the passenger the most using the <em>coef_ </em>function and gave a 81.26% accuracy on the testing\u00a0data.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Lfox6gLs2kiK6WBi89TUKQ.png\"><figcaption>Sex and passenger class were the most important features for predicting the survival rate of the passenger</figcaption></figure><p>Support Vector\u00a0Machines</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FdApk7pbgyI2H_KM2ieR3w.png\"><figcaption>Support vector machines (SVC) classifier</figcaption></figure><p>K-Nearest Neighbors</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6Zv4RCqdtVC8h8Fr6QJQ6w.png\"><figcaption>K-Nearest Neighbors classifier</figcaption></figure><p>Random Forest</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-6MXonKTfqsvQq7mMpFO9g.png\"><figcaption>Random Decision Tree Forest Classifier</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/326/1*k8DZuWUvK-GrE5fpKPeSMw.png\"><figcaption>Model accuracy for each predictive model</figcaption></figure><p>Finally, after building different predictive models, I came up with the following table which show the accuracy scores for each type of classifier. In this case, because the number of features is pretty low and the Sex feature dominating the survival rate, the decision tree classifier was more accurate than the random forest classifier.</p>\n<p>Now, its time to get the predictions out to a csv file and submit some predictions.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tvYy7iNU_9grWhwOia3VFg.png\"><figcaption>To csv method outputs a dataframe into a csv file used by the competition for submissions</figcaption></figure><p>So there\u2019s a lot more to a typical data science notebook than this. I haven\u2019t made optimized my classifiers, I have not tested the data against a validation set after optimization. I hope to create another Medium post soon with all the nice\u00a0stuff.</p>\n<p>Now, that concludes the analysis I have done for the Titanic dataset, you can find the Jupyter notebook at this <a href=\"https://github.com/raghav96/datascience/blob/master/Titanic%20Dataset%20Kaggle%20Competition.ipynb\">link</a>. Feel free to comment on the notebook if you have any questions. Have a great\u00a0day!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=68e50263c14d\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>As the hot summer is upon us, I decided to venture out into a analysis of data from the ice cold depths of the Northern Atlantic Ocean. This article is moderately rigorous and goes through each step I have taken to build this analysis of the titanic dataset. I built this analysis with help from the <a href=\"https://www.kaggle.com/startupsci/titanic-data-science-solutions/\">Titanic Data Science Solutions</a> kernel. The dataset was obtained from Kaggle at this <a href=\"https://www.kaggle.com/c/titanic\">link</a>. Tldr; get the Jupyter notebook from this analysis\u00a0<a href=\"https://github.com/raghav96/datascience/blob/master/Titanic%20Dataset%20Kaggle%20Competition.ipynb\">here</a>.</p>\n<p>In the early hours of 15 April 1912, the RMS Titanic had sunk on collision with an iceberg in its maiden voyage from Southampton to New York City. There were an estimated 2224 passengers on board, and more than 1500 died, making it one of the worst passenger ship disasters in\u00a0history.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*Y3d3HF03WSE6nMzj\"><figcaption>\u201cA cruise ship on a mountain lake with heavy clouds above\u201d by <a href=\"https://unsplash.com/@steinart?utm_source=medium&amp;utm_medium=referral\">Steinar Engeland</a> on\u00a0<a href=\"https://unsplash.com/?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption></figure><p>So given that, lets take a closer look at the people who survived this disaster and predict the likelihood of survival.</p>\n<p>For this analysis, I would be using python packages like pandas, seaborn and numpy and jupyter notebooks.</p>\n<p>Initially I loaded up the dataset onto the Jupyter notebook, using the command <em>pandas.read_csv(file)</em> and imported the data into the notebook. Then I proceeded to review the structure of the dataset to understand what features we can build our model\u00a0on.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*crP8o_KXj02q-zzpE8TFXg.png\"></figure><p>The variables that we had provided had the following descriptors to mean what they\u00a0were:</p>\n<ul>\n<li>Survived: Survived (1) or died\u00a0(0)</li>\n<li>Pclass: Passenger\u2019s class</li>\n<li>Name: Passenger\u2019s name</li>\n<li>Sex: Passenger\u2019s sex</li>\n<li>Age: Passenger\u2019s age</li>\n<li>SibSp: Number of siblings/spouses aboard</li>\n<li>Parch: Number of parents/children aboard</li>\n<li>Ticket: Ticket\u00a0number</li>\n<li>Fare: Fare</li>\n<li>Cabin: Cabin</li>\n<li>Embarked: Port of embarkation</li>\n</ul>\n<p>Trying to find out how much each feature contributes to the survival rate, I first tried to find out the expect survival for 4 features:</p>\n<ul>\n<li>Passenger Class or PClass\u200a\u2014\u200aUsually, higher class (class 1) meant a more expensive fare and the other classes paid lesser to be on the\u00a0ship</li>\n<li>Sex\u200a\u2014\u200aWhat was the survival rate of\u00a0women?</li>\n<li>SibSp\u200a\u2014\u200aWhat was the variation of survival rate based on number of siblings the passenger had on the\u00a0ship?</li>\n<li>Parch\u200a\u2014\u200aWhat was the variation of survival rate based on number of parents/children the passenger had on the\u00a0ship?</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qbCWQcpuArUOQLkfCLGmsQ.png\"><figcaption>Variation with class and gender, women made it alive on the Titanic way more than men because women and children were provided with lifeboats first when the Titanic was\u00a0sinking</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*hlUV9v07NaComUBkCCDBfQ.png\"><figcaption>Percentages of survival for number of siblings and number of parents/children</figcaption></figure><p>Now, these percentages and numbers are likely to bore anyone. So lets make some plots and graphs on the survival rates. You can get the code for this on the notebook at the end of this\u00a0post</p>\n<p>So here\u2019s a plot of variation of survival amongst people of different age. The graph on the left indicates the number of people who did not survive and the graph on the right indicates the number of people who survived the Titanic disaster. As we can see from the graph on the right, infants and toddlers (ages 0\u20134) had a bigger survival rate compared to the children and teenagers (ages 5\u201316). Pretty sad stuff but the data doesn\u2019t lie or hide these things from\u00a0us.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/940/1*i3Hnp60OdUbs7XKarvcn6Q.png\"><figcaption>Variation of death or survival with\u00a0age</figcaption></figure><p>Now lets take a look at the variation of age amongst people who survived/did not survive based on the passenger class they were in. What is interesting in this data is that people who were in the last passenger class (class=3) and who were between the ages of 16\u201340 did not survive the Titanic sinking. So I guess Jack from the movie Titanic was statistically very likely to die in the disaster.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0A1B2Hv8fzkUkmEfuya9Wg.png\"><figcaption>Variation of survival and death with age amongst passengers of different passenger classes</figcaption></figure><p>Let\u2019s see how much these passengers paid to be on the Titanic and where they started the journey from. And if there is any correlation between the money they paid and surviving the Titanic disaster. So there seems to be no correlation of survival between fare paid and the likeliness or survival amongst the three different embarking cities.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*4sM8L_P8tcXbpC0FC8lkvw.png\"></figure><p>So let\u2019s start the real analysis here. Here I am trying to build a predictive model based on the In this analysis I used different feature extraction techniques to build a encoding vector for the dataset I am provided. Firstly, I tried to convert a simple category into a binary feature. I took a simple one like Sex and converted the values into 0 for male and 1 for female in this\u00a0way.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*JspPX9QNiaE7xHFL5OUWfg.png\"><figcaption>Converting the Sex column which was text into a binary 0 or 1 numerical column</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/334/1*iqUUHne68Dr3gOK3rbdPjw.png\"></figure><p>Then, I wanted to try a more complex feature from the names of the passengers. So I tried to extract the title from the names of the passengers and ended up with this on the right. Here I found out some of these titles were\u00a0similar:</p>\n<ul>\n<li>Mlle means \u2018Mademoiselle\u2019 which is the honorific used to describe someone who would be called \u2018Miss\u2019 in\u00a0France</li>\n<li>Mme means \u2018Madame\u2019 which is the honorific used to describe someone who would be called \u2018Mrs\u2019 in\u00a0France</li>\n<li>Ms is the short form for\u00a0\u2018Miss\u2019</li>\n<li>Lady, Countess, Dona are female honorifics of\u00a0nobility</li>\n<li>Don, Sir, Jonkheer are male honorifics of\u00a0nobility</li>\n<li>Capt refers to the Captain of the Titanic; Col, Major are military positions; Dr are doctors, Rev is a Reverend who all have special roles in\u00a0society</li>\n</ul>\n<p>Based on that, I came up with this list of titles and the chances of survival for each\u00a0title</p>\n<ul>\n<li>Master\u200a\u2014\u200aused to denote someone\u00a0younger</li>\n<li>Miss\u200a\u2014\u200ayoung\u00a0women</li>\n<li>Mr\u200a\u2014\u200amen</li>\n<li>Mrs\u200a\u2014\u200amarried\u00a0women</li>\n<li>Royalty\u200a\u2014\u200apeople with fancy\u00a0titles</li>\n<li>Special\u200a\u2014\u200athe captain, doctors and reverends who might have been called onto help during the\u00a0disaster</li>\n</ul>\n<p>On extracting them from the database and building a title feature we get survival rates based on each title. Looks like a lot of women survived, especially ones that were married. Titanic was a big widowmaker.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*T4i08wh_dzJCRM0ndZgw8g.png\"></figure><p>Now there is another thing we should look to do when we are building a predictive model, fill in values to fill columns with null values. Now let us analyze the Age column in order to fill null values with a predicted the value and build our feature from\u00a0that.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_nRnzIR5O-jpfAbnog_lYA.png\"><figcaption>There are a 177 rows with null values in the Age\u00a0column</figcaption></figure><p>So for filling in the null values in columns with predicted values, let us calculate the median age for passengers in each passenger class and sex; and fill in the age values for the 177 rows based on their passenger class and\u00a0sex.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lv8V5adGH22vR8IqTxCoIQ.png\"><figcaption>Calculating the median age by passenger class and\u00a0sex</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*8yZnHUvll4jWeeF_SBNUwA.png\"><figcaption>Filling in the null values in the age column to get a complete Age\u00a0column</figcaption></figure><p>Now since we have filled up all the null values, the Age column has become a continuous range of numerical values. We could convert that into a feature by creating bands of values that we define. Here I chose my age groups as the following:</p>\n<ul>\n<li>Age 16 and\u00a0under</li>\n<li>Ages 16\u201332</li>\n<li>Ages 32\u201348</li>\n<li>Ages 48\u201364</li>\n<li>Ages 64 and\u00a0above.</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Nv9befZsIYfeDBKE01Pavw.png\"><figcaption>Building a feature based on Age\u00a0group</figcaption></figure><p>Now, I followed up to build a family size feature by adding the two columns that were provided to us to create a new column, SibSp\u200a\u2014\u200awhich denotes the number of siblings who were also passengers and Parch\u200a\u2014\u200awhich denotes the number of parents/children.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*GJhD3tdoHGQDMWeI1sHCNg.png\"><figcaption>FamilySize feature being built by combining the number of siblings and parents, sorted for survival rate. Looks like 4 member families are good for surviving sinking passenger ships.</figcaption></figure><p>So I built a couple more features like this, by making categorical columns into numeric features with the Embarked column and eventually ended up with this as my training and testing\u00a0data.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-L-rkc_-FbXMPGvpfigxtA.png\"><figcaption>Final cleaned and feature extracted training and test\u00a0dataset</figcaption></figure><p>Now, over to trying out different basic predictive models on the training and testing data. In order to build our analysis, I am going to use the following predictive models:</p>\n<ul>\n<li>Logistic Regression</li>\n<li>Support Vector\u00a0Machines</li>\n<li>KNN or K-Nearest Neighbors</li>\n<li>Decision Trees</li>\n<li>Random Forest</li>\n</ul>\n<p>Logistic Regression is useful to see which features affect the survival rate of the passenger the most using the <em>coef_ </em>function and gave a 81.26% accuracy on the testing\u00a0data.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Lfox6gLs2kiK6WBi89TUKQ.png\"><figcaption>Sex and passenger class were the most important features for predicting the survival rate of the passenger</figcaption></figure><p>Support Vector\u00a0Machines</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*FdApk7pbgyI2H_KM2ieR3w.png\"><figcaption>Support vector machines (SVC) classifier</figcaption></figure><p>K-Nearest Neighbors</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6Zv4RCqdtVC8h8Fr6QJQ6w.png\"><figcaption>K-Nearest Neighbors classifier</figcaption></figure><p>Random Forest</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*-6MXonKTfqsvQq7mMpFO9g.png\"><figcaption>Random Decision Tree Forest Classifier</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/326/1*k8DZuWUvK-GrE5fpKPeSMw.png\"><figcaption>Model accuracy for each predictive model</figcaption></figure><p>Finally, after building different predictive models, I came up with the following table which show the accuracy scores for each type of classifier. In this case, because the number of features is pretty low and the Sex feature dominating the survival rate, the decision tree classifier was more accurate than the random forest classifier.</p>\n<p>Now, its time to get the predictions out to a csv file and submit some predictions.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*tvYy7iNU_9grWhwOia3VFg.png\"><figcaption>To csv method outputs a dataframe into a csv file used by the competition for submissions</figcaption></figure><p>So there\u2019s a lot more to a typical data science notebook than this. I haven\u2019t made optimized my classifiers, I have not tested the data against a validation set after optimization. I hope to create another Medium post soon with all the nice\u00a0stuff.</p>\n<p>Now, that concludes the analysis I have done for the Titanic dataset, you can find the Jupyter notebook at this <a href=\"https://github.com/raghav96/datascience/blob/master/Titanic%20Dataset%20Kaggle%20Competition.ipynb\">link</a>. Feel free to comment on the notebook if you have any questions. Have a great\u00a0day!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=68e50263c14d\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["towards-data-science","data-science"]}]}